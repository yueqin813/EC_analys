{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd86022-609a-4504-bb19-f515224d3e9d",
   "metadata": {},
   "source": [
    "# 'Basic_statistics.ipynb' is created by Yue on Feb 8, 2024 for computing basic statistics.\n",
    "\n",
    "Workflow:\n",
    "1. Load original instant data of ux,uy,uz after data_processing and load hourly, qc files.\n",
    "2. Combine all wind_angle files, qc files, datetime into 1 single file and get the length (number of hours).\n",
    "3. Compute air density.\n",
    "4. Compute avg, std, fluxes.\n",
    "5. Compute stability parameters.\n",
    "6. Save results.\n",
    "\n",
    "Notes:\n",
    "1. input directory:/save_processed_data.\n",
    "2. output directory: /save_statistical_data.\n",
    "3. P and T have no value at 3, 5, 7, 11 level.\n",
    "4. Statistical results will be saved separately in a single file with a length equal to the total number of hours.\n",
    "5. 'basic_statistics' dataframe contains hourly statistical variables.\n",
    "6. Last 9 hours have no values in all variables.\n",
    "7. datetime_all files contain hours from 0 to 23.\n",
    "\n",
    "=========== Disable de-spike on Sep 14, 2024 and save data to /save_statistical_data_091424 =====\n",
    "\n",
    "=========== Disable de-spike and do planar fit on Sep 19, 2024 and save data to /save_statistical_data_planarfit =====\n",
    "\n",
    "=========== Disable de-spike and do planar fit on Sep 19, 2024 and save data to /save_statistical_data_092024 ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcd694-5d1a-4bdf-8d88-dcac3141c469",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b91124-47df-43cc-907d-64314aa0f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "from matplotlib.pyplot import figure\n",
    "import scipy.io as sio\n",
    "from datetime import date, timedelta\n",
    "from math import *\n",
    "from scipy.stats import gmean\n",
    "from scipy import ndimage\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec57a3-980b-4c6c-b50a-93352c947ce2",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0783d12b-0bfb-4e7b-8349-4fa49c7b5b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: /projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_processed_data_092024/\n",
      "Directory exists: /projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_statistical_data_092024/\n"
     ]
    }
   ],
   "source": [
    "# directories\n",
    "IN_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_processed_data_092024/\"\n",
    "OUT_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_statistical_data_092024/\"\n",
    "\n",
    "# List of directories to check\n",
    "directories = [IN_DIR, OUT_DIR]\n",
    "\n",
    "# Check if directories exist, create them if they don't\n",
    "for dir_path in directories:\n",
    "    if not os.path.exists(dir_path):\n",
    "        try:\n",
    "            os.makedirs(dir_path)\n",
    "            print(f\"Created directory: {dir_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating directory {dir_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Directory exists: {dir_path}\")\n",
    "\n",
    "# physical constants (or values that are assumed constant)\n",
    "Rw  = 461.5     # ideal gas constant for water vapor, J/kg*K\n",
    "Rd  = 287.05    # ideal gas constant for dry air, J/kg*K\n",
    "Lv  = 1000*2257 # latent heat of vaporization (water), J/kg\n",
    "Cp  = 1005      # approximate constant pressure specific heat of air, J/kg*K\n",
    "k   = 0.4      # Von Karman constant\n",
    "g   = 9.81      # acceleration of gravity, m/s^2\n",
    "\n",
    "# global constants\n",
    "sonum    =12                       # number of sonic\n",
    "z  = np.array([1.2,2,3.5,6,9,12.5,16.5,23,30,40,50,60])  # height of sonic above ground, \n",
    "frequency=10                   # sampling rate, Hz\n",
    "time_avg =3600                  # average time, s\n",
    "rpat = time_avg*frequency           # number of lines for a loop\n",
    "\n",
    "# input variables\n",
    "in_tur = ['u_dspk_2rot_ldtr', 'v_dspk_2rot_ldtr', 'w_dspk_2rot_ldtr', 'T_dspk_ldtr', \n",
    "          'u_dspk_2rot_filt', 'v_dspk_2rot_filt', 'w_dspk_2rot_filt', 'T_dspk_filt', \n",
    "          'q_ins_rnan','P_ins_rnan','T_dspk_ldtr']\n",
    "in_other = ['ts_dspk_wind_ang', 'u_filt_size']\n",
    "in_qf = ['qc_ux_nan', 'qc_uy_nan', 'qc_uz_nan', 'qc_T_nan', 'qc_q_nan', 'qc_P_nan',\n",
    "         'qc_ux_dspk', 'qc_uy_dspk', 'qc_uz_dspk', 'qc_T_dspk', 'qc_wdir_dspk']\n",
    "in_nspikes = ['u_nspikes', 'v_nspikes', 'w_nspikes', 'T_nspikes']\n",
    "\n",
    "# output variables\n",
    "out_other = ['wind_ang_all', 'filter_size_all','n_hours','datetime_all']\n",
    "out_qf = ['qc_ux_nan_all', 'qc_uy_nan_all', 'qc_uz_nan_all', 'qc_T_nan_all', 'qc_q_nan_all', 'qc_P_nan_all',\n",
    "         'qc_ux_dspk_all', 'qc_uy_dspk_all', 'qc_uz_dspk_all', 'qc_T_dspk_all', 'qc_wdir_dspk_all']\n",
    "out_nspikes = ['u_nspikes_all', 'v_nspikes_all', 'w_nspikes_all', 'T_nspikes_all']\n",
    "\n",
    "out_tur = ['u_tur_ldtr', 'v_tur_ldtr', 'w_tur_ldtr', 'T_tur_ldtr',\n",
    "           'u_tur_filt', 'v_tur_filt', 'w_tur_filt', 'T_tur_filt']\n",
    "out_avg = ['P_avg', 'T_avg', 'q_avg', 'Rho_air', \n",
    "           'u_avg_ldtr', 'v_avg_ldtr', 'w_avg_ldtr', 'T_avg_ldtr', \n",
    "           'u_avg_filt', 'v_avg_filt', 'w_avg_filt', 'T_avg_filt']\n",
    "out_std = ['u_std_ldtr', 'v_std_ldtr', 'w_std_ldtr', 'T_std_ldtr',\n",
    "           'u_std_filt', 'v_std_filt', 'w_std_filt', 'T_std_filt']\n",
    "out_flux = ['uw_ldtr', 'vw_ldtr', 'wT_ldtr', 'u_star_ldtr', 'H_ldtr', \n",
    "            'uw_filt', 'vw_filt', 'wT_filt', 'u_star_filt', 'H_filt']\n",
    "out_stability = ['L_H2_ldtr', 'stability_ldtr', 'L_H2_filt', 'stability_filt']\n",
    "\n",
    "# controls\n",
    "# webb_corr = 2 # do webb-corr on q and C only\n",
    "\n",
    "# labels\n",
    "list_bot = np.array([0,1,2,3,4]) # bottom five levels\n",
    "list_sel_m2 = np.array([5,6,7,8,9,10]) # from level 6 to level 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcdbd4-c9c2-4990-95b7-33a4d4eaa372",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66cdd7f-a500-42ef-bd58-7ac3800cfc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_list(sdate,edate):\n",
    "    \"\"\"method used for creating date list\"\"\"\n",
    "    delta = edate - sdate       # as timedelta\n",
    "    day = [sdate+timedelta(days=x) for x in range(delta.days+1)]\n",
    "    return day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b55655-05d2-432d-82f3-50def5c213ba",
   "metadata": {},
   "source": [
    "# Do the statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67595cce-e168-457f-be8f-5c440167ce2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_statistical_data_092024/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up time period and initialize variables\n",
    "Sdate = date(2020,9,25)\n",
    "# Sdate = date(2020,10,17)\n",
    "Edate = date(2021,4,23)\n",
    "# Edate = date(2020,9,26)\n",
    "ds = date_list(Sdate,Edate)\n",
    "write_results = True\n",
    "OUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1aa0ac-4b61-4d56-8347-be6b52555088",
   "metadata": {},
   "source": [
    "# Combine some arrays into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aa51719-c477-4057-8a2a-7a9088248fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201216 do not exist\n",
      "5040\n",
      "CPU times: user 1.49 s, sys: 363 ms, total: 1.85 s\n",
      "Wall time: 3.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize lists to store the combined data\n",
    "wind_ang_combined = []\n",
    "filter_size_combined = []\n",
    "qc_ux_nan_combined = []\n",
    "qc_uy_nan_combined = []\n",
    "qc_uz_nan_combined = []\n",
    "qc_T_nan_combined = []\n",
    "qc_q_nan_combined = []\n",
    "qc_P_nan_combined = []\n",
    "\n",
    "qc_ux_dspk_combined = []\n",
    "qc_uy_dspk_combined = []\n",
    "qc_uz_dspk_combined = []\n",
    "qc_T_dspk_combined = []\n",
    "\n",
    "u_nspikes_combined = []\n",
    "v_nspikes_combined = []\n",
    "w_nspikes_combined = []\n",
    "T_nspikes_combined = []\n",
    "\n",
    "qc_wdir_dspk_combined = []\n",
    "datetime_all = []\n",
    "# combine arrays to a single file\n",
    "for day in ds:\n",
    "    strday = str(day.strftime(\"%Y%m%d\"))\n",
    "    fp_stats = f\"{IN_DIR}{'ts_dspk_wind_ang'}_{strday}.npy\"\n",
    "    if (not os.path.isfile(fp_stats)):\n",
    "        print(day.strftime(\"%Y%m%d\")+' do not exist')\n",
    "        continue\n",
    "    # print('start processing:'+ strday)\n",
    "    # load daily data    \n",
    "    for var_name in in_other+in_qf+in_nspikes:\n",
    "        globals()[var_name] = np.load(f\"{IN_DIR}{var_name}_{strday}.npy\")\n",
    "    wind_ang_combined.append(ts_dspk_wind_ang)\n",
    "    filter_size_combined.append(u_filt_size)\n",
    "    qc_ux_nan_combined.append(qc_ux_nan)\n",
    "    qc_uy_nan_combined.append(qc_uy_nan)\n",
    "    qc_uz_nan_combined.append(qc_uz_nan)\n",
    "    qc_T_nan_combined.append(qc_T_nan)\n",
    "    qc_q_nan_combined.append(qc_q_nan)\n",
    "    qc_P_nan_combined.append(qc_P_nan)\n",
    "    \n",
    "    qc_ux_dspk_combined.append(qc_ux_dspk)\n",
    "    qc_uy_dspk_combined.append(qc_uy_dspk)\n",
    "    qc_uz_dspk_combined.append(qc_uz_dspk)\n",
    "    qc_T_dspk_combined.append(qc_T_dspk)\n",
    "    \n",
    "    u_nspikes_combined.append(u_nspikes)\n",
    "    v_nspikes_combined.append(v_nspikes)\n",
    "    w_nspikes_combined.append(w_nspikes)\n",
    "    T_nspikes_combined.append(T_nspikes)\n",
    "    \n",
    "    qc_wdir_dspk_combined.append(qc_wdir_dspk)\n",
    "    for ih in range(24):\n",
    "        datetime_all.append([day,ih])\n",
    "    # end of the loop\n",
    "    \n",
    "# Reframe lists to nparrays.\n",
    "wind_ang_all = np.vstack(wind_ang_combined)  # vertically stacking arrays\n",
    "filter_size_all = np.vstack(filter_size_combined)\n",
    "qc_ux_nan_all = np.vstack(qc_ux_nan_combined)\n",
    "qc_uy_nan_all = np.vstack(qc_uy_nan_combined)\n",
    "qc_uz_nan_all = np.vstack(qc_uz_nan_combined)\n",
    "qc_T_nan_all = np.vstack(qc_T_nan_combined)\n",
    "qc_q_nan_all = np.vstack(qc_q_nan_combined)\n",
    "qc_P_nan_all = np.vstack(qc_P_nan_combined)\n",
    "\n",
    "qc_ux_dspk_all = np.vstack(qc_ux_dspk_combined)\n",
    "qc_uy_dspk_all = np.vstack(qc_uy_dspk_combined)\n",
    "qc_uz_dspk_all = np.vstack(qc_uz_dspk_combined)\n",
    "qc_T_dspk_all = np.vstack(qc_T_dspk_combined)\n",
    "\n",
    "u_nspikes_all = np.vstack(u_nspikes_combined)\n",
    "v_nspikes_all = np.vstack(v_nspikes_combined)\n",
    "w_nspikes_all = np.vstack(w_nspikes_combined)\n",
    "T_nspikes_all = np.vstack(T_nspikes_combined)\n",
    "\n",
    "qc_wdir_dspk_all = np.vstack(qc_wdir_dspk_combined)\n",
    "datetime_all = np.array(datetime_all)\n",
    "n_hours = len(wind_ang_all)\n",
    "print(n_hours)\n",
    "\n",
    "# save combined arrays\n",
    "if write_results: # write output by days\n",
    "    for var_name in out_other+out_qf+out_nspikes:\n",
    "    # for var_name in ['datetime_all']:\n",
    "        var_value = globals()[var_name]\n",
    "        filename = f\"{var_name}.npy\"\n",
    "        np.save(OUT_DIR + filename, var_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a079f96-1db1-4e28-8138-47aee30d2238",
   "metadata": {},
   "source": [
    "# Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d83452-b444-4ed1-b0c0-a3f76335adf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_statistical_data_092024/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up time period and initialize variables\n",
    "Sdate = date(2020,9,25)\n",
    "# Sdate = date(2020,10,17)\n",
    "Edate = date(2021,4,23)\n",
    "# Edate = date(2020,9,26)\n",
    "ds = date_list(Sdate,Edate)\n",
    "n_hours = np.load(f\"{OUT_DIR}{'n_hours'}.npy\")\n",
    "# Do statistics\n",
    "# initialization\n",
    "## avg variables\n",
    "P_avg = np.zeros((n_hours,sonum)) * np.nan\n",
    "T_avg = np.zeros((n_hours,sonum)) * np.nan\n",
    "q_avg = np.zeros((n_hours,sonum)) * np.nan\n",
    "Rho_air = np.zeros((n_hours,sonum)) * np.nan\n",
    "\n",
    "u_avg_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "v_avg_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "w_avg_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "T_avg_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "\n",
    "u_avg_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "v_avg_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "w_avg_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "T_avg_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "\n",
    "## std\n",
    "u_std_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "v_std_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "w_std_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "T_std_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "\n",
    "u_std_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "v_std_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "w_std_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "T_std_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "\n",
    "## fluxes\n",
    "uw_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "vw_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "wT_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "u_star_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "H_ldtr = np.zeros((n_hours,sonum)) * np.nan\n",
    "\n",
    "uw_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "vw_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "wT_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "u_star_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "H_filt = np.zeros((n_hours,sonum)) * np.nan\n",
    "\n",
    "## other variables\n",
    "L_H2_ldtr = np.zeros((n_hours,sonum)) * np.nan ### Obukhov length\n",
    "stability_ldtr  = np.zeros((n_hours,sonum)) * np.nan\n",
    "L_H2_filt = np.zeros((n_hours,sonum)) * np.nan \n",
    "stability_filt  = np.zeros((n_hours,sonum)) * np.nan\n",
    "# qc_wdir_dspk_all = np.zeros((n_hours,sonum)) * np.nan\n",
    "\n",
    "write_results = True\n",
    "OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbac78ca-50c1-4042-9b70-00b7837e012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing:20200925\n",
      "start processing:20200926\n",
      "start processing:20200927\n",
      "start processing:20200928\n",
      "start processing:20200929\n",
      "start processing:20200930\n",
      "start processing:20201001\n",
      "start processing:20201002\n",
      "start processing:20201003\n",
      "start processing:20201004\n",
      "start processing:20201005\n",
      "start processing:20201006\n",
      "start processing:20201007\n",
      "start processing:20201008\n",
      "start processing:20201009\n",
      "start processing:20201010\n",
      "start processing:20201011\n",
      "start processing:20201012\n",
      "start processing:20201013\n",
      "start processing:20201014\n",
      "start processing:20201015\n",
      "start processing:20201016\n",
      "start processing:20201017\n",
      "start processing:20201018\n",
      "start processing:20201019\n",
      "start processing:20201020\n",
      "start processing:20201021\n",
      "start processing:20201022\n",
      "start processing:20201023\n",
      "start processing:20201024\n",
      "start processing:20201025\n",
      "start processing:20201026\n",
      "start processing:20201027\n",
      "start processing:20201028\n",
      "start processing:20201029\n",
      "start processing:20201030\n",
      "start processing:20201031\n",
      "start processing:20201101\n",
      "start processing:20201102\n",
      "start processing:20201103\n",
      "start processing:20201104\n",
      "start processing:20201105\n",
      "start processing:20201106\n",
      "start processing:20201107\n",
      "start processing:20201108\n",
      "start processing:20201109\n",
      "start processing:20201110\n",
      "start processing:20201111\n",
      "start processing:20201112\n",
      "start processing:20201113\n",
      "start processing:20201114\n",
      "start processing:20201115\n",
      "start processing:20201116\n",
      "start processing:20201117\n",
      "start processing:20201118\n",
      "start processing:20201119\n",
      "start processing:20201120\n",
      "start processing:20201121\n",
      "start processing:20201122\n",
      "start processing:20201123\n",
      "start processing:20201124\n",
      "start processing:20201125\n",
      "start processing:20201126\n",
      "start processing:20201127\n",
      "start processing:20201128\n",
      "start processing:20201129\n",
      "start processing:20201130\n",
      "start processing:20201201\n",
      "start processing:20201202\n",
      "start processing:20201203\n",
      "start processing:20201204\n",
      "start processing:20201205\n",
      "start processing:20201206\n",
      "start processing:20201207\n",
      "start processing:20201208\n",
      "start processing:20201209\n",
      "start processing:20201210\n",
      "start processing:20201211\n",
      "start processing:20201212\n",
      "start processing:20201213\n",
      "start processing:20201214\n",
      "start processing:20201215\n",
      "20201216 do not exist\n",
      "start processing:20201217\n",
      "start processing:20201218\n",
      "start processing:20201219\n",
      "start processing:20201220\n",
      "start processing:20201221\n",
      "start processing:20201222\n",
      "start processing:20201223\n",
      "start processing:20201224\n",
      "start processing:20201225\n",
      "start processing:20201226\n",
      "start processing:20201227\n",
      "start processing:20201228\n",
      "start processing:20201229\n",
      "start processing:20201230\n",
      "start processing:20201231\n",
      "start processing:20210101\n",
      "start processing:20210102\n",
      "start processing:20210103\n",
      "start processing:20210104\n",
      "start processing:20210105\n",
      "start processing:20210106\n",
      "start processing:20210107\n",
      "start processing:20210108\n",
      "start processing:20210109\n",
      "start processing:20210110\n",
      "start processing:20210111\n",
      "start processing:20210112\n",
      "start processing:20210113\n",
      "start processing:20210114\n",
      "start processing:20210115\n",
      "start processing:20210116\n",
      "start processing:20210117\n",
      "start processing:20210118\n",
      "start processing:20210119\n",
      "start processing:20210120\n",
      "start processing:20210121\n",
      "start processing:20210122\n",
      "start processing:20210123\n",
      "start processing:20210124\n",
      "start processing:20210125\n",
      "start processing:20210126\n",
      "start processing:20210127\n",
      "start processing:20210128\n",
      "start processing:20210129\n",
      "start processing:20210130\n",
      "start processing:20210131\n",
      "start processing:20210201\n",
      "start processing:20210202\n",
      "start processing:20210203\n",
      "start processing:20210204\n",
      "start processing:20210205\n",
      "start processing:20210206\n",
      "start processing:20210207\n",
      "start processing:20210208\n",
      "start processing:20210209\n",
      "start processing:20210210\n",
      "start processing:20210211\n",
      "start processing:20210212\n",
      "start processing:20210213\n",
      "start processing:20210214\n",
      "start processing:20210215\n",
      "start processing:20210216\n",
      "start processing:20210217\n",
      "start processing:20210218\n",
      "start processing:20210219\n",
      "start processing:20210220\n",
      "start processing:20210221\n",
      "start processing:20210222\n",
      "start processing:20210223\n",
      "start processing:20210224\n",
      "start processing:20210225\n",
      "start processing:20210226\n",
      "start processing:20210227\n",
      "start processing:20210228\n",
      "start processing:20210301\n",
      "start processing:20210302\n",
      "start processing:20210303\n",
      "start processing:20210304\n",
      "start processing:20210305\n",
      "start processing:20210306\n",
      "start processing:20210307\n",
      "start processing:20210308\n",
      "start processing:20210309\n",
      "start processing:20210310\n",
      "start processing:20210311\n",
      "start processing:20210312\n",
      "start processing:20210313\n",
      "start processing:20210314\n",
      "start processing:20210315\n",
      "start processing:20210316\n",
      "start processing:20210317\n",
      "start processing:20210318\n",
      "start processing:20210319\n",
      "start processing:20210320\n",
      "start processing:20210321\n",
      "start processing:20210322\n",
      "start processing:20210323\n",
      "start processing:20210324\n",
      "start processing:20210325\n",
      "start processing:20210326\n",
      "start processing:20210327\n",
      "start processing:20210328\n",
      "start processing:20210329\n",
      "start processing:20210330\n",
      "start processing:20210331\n",
      "start processing:20210401\n",
      "start processing:20210402\n",
      "start processing:20210403\n",
      "start processing:20210404\n",
      "start processing:20210405\n",
      "start processing:20210406\n",
      "start processing:20210407\n",
      "start processing:20210408\n",
      "start processing:20210409\n",
      "start processing:20210410\n",
      "start processing:20210411\n",
      "start processing:20210412\n",
      "start processing:20210413\n",
      "start processing:20210414\n",
      "start processing:20210415\n",
      "start processing:20210416\n",
      "start processing:20210417\n",
      "start processing:20210418\n",
      "start processing:20210419\n",
      "start processing:20210420\n",
      "start processing:20210421\n",
      "start processing:20210422\n",
      "start processing:20210423\n",
      "CPU times: user 15min 2s, sys: 4min 54s, total: 19min 57s\n",
      "Wall time: 52min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_hr = 0\n",
    "for day in ds:\n",
    "    strday = str(day.strftime(\"%Y%m%d\"))\n",
    "    fp_stats = f\"{IN_DIR}{'ts_dspk_wind_ang'}_{strday}.npy\"\n",
    "    if (not os.path.isfile(fp_stats)):\n",
    "        print(day.strftime(\"%Y%m%d\")+' do not exist')\n",
    "        continue\n",
    "\n",
    "    ##-------------------------------------------------\n",
    "    print(f\"start processing:{strday}\")\n",
    "    # start the loop of hours\n",
    "    for ih in range(24): \n",
    "        # initialize tur data at every hour\n",
    "        u_tur_ldtr = np.zeros((rpat,sonum)) * np.nan\n",
    "        v_tur_ldtr = np.zeros((rpat,sonum)) * np.nan\n",
    "        w_tur_ldtr = np.zeros((rpat,sonum)) * np.nan\n",
    "        T_tur_ldtr = np.zeros((rpat,sonum)) * np.nan \n",
    "\n",
    "        u_tur_filt = np.zeros((rpat,sonum)) * np.nan\n",
    "        v_tur_filt = np.zeros((rpat,sonum)) * np.nan\n",
    "        w_tur_filt = np.zeros((rpat,sonum)) * np.nan\n",
    "        T_tur_filt = np.zeros((rpat,sonum)) * np.nan\n",
    "        # load tur data\n",
    "        for var_name in in_tur:\n",
    "            globals()[var_name] = np.load(f\"{IN_DIR}{var_name}_{strday}_{ih:02}00.npy\")\n",
    "        \n",
    "        # calculate air density by ideal gas law\n",
    "        P_avg[ih+count_hr,:] = np.nanmean(P_ins_rnan,axis=0) \n",
    "        T_avg[ih+count_hr,:] = np.nanmean(T_dspk_ldtr,axis=0)\n",
    "        q_avg[ih+count_hr,:] = np.nanmean(q_ins_rnan,axis=0)\n",
    "        Rho_air[ih+count_hr,:] = P_avg[ih+count_hr,:]/(287.04*T_avg[ih+count_hr,:]) - 0.61*q_avg[ih+count_hr,:]\n",
    "        # fill in missing data at 3rd, 5th, 7th, 11th levels\n",
    "        Rho_air[ih+count_hr,2] = 0.5*(Rho_air[ih+count_hr,1] + Rho_air[ih+count_hr,3])\n",
    "        Rho_air[ih+count_hr,4] = 0.5*(Rho_air[ih+count_hr,3] + Rho_air[ih+count_hr,5])   \n",
    "        Rho_air[ih+count_hr,6] = 0.5*(Rho_air[ih+count_hr,5] + Rho_air[ih+count_hr,7])\n",
    "        Rho_air[ih+count_hr,10] = 0.5*(Rho_air[ih+count_hr,9] + Rho_air[ih+count_hr,11]) \n",
    "        \n",
    "        # Calculate statistics\n",
    "        # start_time = time.time()\n",
    "        ## Calculate mean variables\n",
    "        u_avg_ldtr[ih+count_hr,:] = np.nanmean(u_dspk_2rot_ldtr,axis=0) # 12*1\n",
    "        v_avg_ldtr[ih+count_hr,:] = np.nanmean(v_dspk_2rot_ldtr,axis=0)\n",
    "        w_avg_ldtr[ih+count_hr,:] = np.nanmean(w_dspk_2rot_ldtr,axis=0)\n",
    "        T_avg_ldtr[ih+count_hr,:] = np.nanmean(T_dspk_ldtr,axis=0)\n",
    "        \n",
    "        u_avg_filt[ih+count_hr,:] = np.nanmean(u_dspk_2rot_filt,axis=0) # 12*1\n",
    "        v_avg_filt[ih+count_hr,:] = np.nanmean(v_dspk_2rot_filt,axis=0)\n",
    "        w_avg_filt[ih+count_hr,:] = np.nanmean(w_dspk_2rot_filt,axis=0)\n",
    "        T_avg_filt[ih+count_hr,:] = np.nanmean(T_dspk_filt,axis=0)\n",
    "        # print('1')\n",
    "        \n",
    "        ## calculate turbulent variables for 1 hr (36000*12)\n",
    "        u_tur_ldtr = u_dspk_2rot_ldtr - u_avg_ldtr[ih+count_hr,:]\n",
    "        v_tur_ldtr = v_dspk_2rot_ldtr - v_avg_ldtr[ih+count_hr,:] \n",
    "        w_tur_ldtr = w_dspk_2rot_ldtr - w_avg_ldtr[ih+count_hr,:]\n",
    "        T_tur_ldtr = T_dspk_ldtr - T_avg_ldtr[ih+count_hr,:] \n",
    "        \n",
    "        u_tur_filt = u_dspk_2rot_filt - u_avg_filt[ih+count_hr,:]\n",
    "        v_tur_filt = v_dspk_2rot_filt - v_avg_filt[ih+count_hr,:]\n",
    "        w_tur_filt = w_dspk_2rot_filt - w_avg_filt[ih+count_hr,:]\n",
    "        T_tur_filt = T_dspk_filt - T_avg_filt[ih+count_hr,:]\n",
    "        \n",
    "        ## calculate standard deviations\n",
    "        u_std_ldtr[ih+count_hr,:] = np.nanstd(u_tur_ldtr,axis=0)\n",
    "        v_std_ldtr[ih+count_hr,:] = np.nanstd(v_tur_ldtr,axis=0)\n",
    "        w_std_ldtr[ih+count_hr,:] = np.nanstd(w_tur_ldtr,axis=0)\n",
    "        T_std_ldtr[ih+count_hr,:] = np.nanstd(T_tur_ldtr,axis=0)\n",
    "        \n",
    "        u_std_filt[ih+count_hr,:] = np.nanstd(u_tur_filt,axis=0)\n",
    "        v_std_filt[ih+count_hr,:] = np.nanstd(v_tur_filt,axis=0)\n",
    "        w_std_filt[ih+count_hr,:] = np.nanstd(w_tur_filt,axis=0)\n",
    "        T_std_filt[ih+count_hr,:] = np.nanstd(T_tur_filt,axis=0)\n",
    "        \n",
    "        ## calcultate covariance and fluxes\n",
    "        uw_ldtr[ih+count_hr,:] = np.nanmean(u_tur_ldtr*w_tur_ldtr,axis=0)\n",
    "        vw_ldtr[ih+count_hr,:] = np.nanmean(v_tur_ldtr*w_tur_ldtr,axis=0)\n",
    "        wT_ldtr[ih+count_hr,:] = np.nanmean(w_tur_ldtr*T_tur_ldtr,axis=0) # sensible heat flux\n",
    "        H_ldtr[ih+count_hr,:]  = Cp*Rho_air[ih+count_hr,:]*wT_ldtr[ih+count_hr,:]\n",
    "        \n",
    "        uw_filt[ih+count_hr,:] = np.nanmean(u_tur_filt*w_tur_filt,axis=0)\n",
    "        vw_filt[ih+count_hr,:] = np.nanmean(v_tur_filt*w_tur_filt,axis=0)\n",
    "        wT_filt[ih+count_hr,:] = np.nanmean(w_tur_filt*T_tur_filt,axis=0) # sensible heat flux\n",
    "        H_filt[ih+count_hr,:]  = Cp*Rho_air[ih+count_hr,:]*wT_filt[ih+count_hr,:]\n",
    "        \n",
    "        u_star_ldtr[ih+count_hr,:] = np.maximum(0,(uw_ldtr[ih+count_hr,:]**2+vw_ldtr[ih+count_hr,:]**2)**0.25)\n",
    "        u_star_filt[ih+count_hr,:] = np.maximum(0,(uw_filt[ih+count_hr,:]**2+vw_filt[ih+count_hr,:]**2)**0.25)\n",
    "        \n",
    "        # end_time = time.time()\n",
    "        # duration = end_time - start_time\n",
    "        # print(f\"Time taken for statistics: {duration} seconds\")\n",
    "        \n",
    "        # calculate other variables\n",
    "        ## Obukhov length (m) without Evaporation term \n",
    "        ## results are very similar between _ldtr and _filt so only keep one\n",
    "        L_H2_ldtr[ih+count_hr,:] = -u_star_ldtr[ih+count_hr,:]**3*T_avg_ldtr[ih+count_hr,:]/(k*g*wT_ldtr[ih+count_hr,:]) \n",
    "        stability_ldtr[ih+count_hr,:] = z/L_H2_ldtr[ih+count_hr,:]\n",
    "        L_H2_filt[ih+count_hr,:] = -u_star_filt[ih+count_hr,:]**3*T_avg_filt[ih+count_hr,:]/(k*g*wT_filt[ih+count_hr,:]) \n",
    "        stability_filt[ih+count_hr,:] = z/L_H2_filt[ih+count_hr,:]\n",
    "        # print('start df')\n",
    "        df = pd.DataFrame({'P_avg':P_avg[ih+count_hr,:], \n",
    "               'T_avg':T_avg[ih+count_hr,:],\n",
    "               'q_avg':q_avg[ih+count_hr,:],\n",
    "               'Rho_air':Rho_air[ih+count_hr,:],\n",
    "               'u_avg_ldtr':u_avg_ldtr[ih+count_hr,:],\n",
    "               'v_avg_ldtr':v_avg_ldtr[ih+count_hr,:],\n",
    "               'w_avg_ldtr':w_avg_ldtr[ih+count_hr,:],\n",
    "               'T_avg_ldtr':T_avg_ldtr[ih+count_hr,:],\n",
    "               'u_avg_filt':u_avg_filt[ih+count_hr,:],\n",
    "               'v_avg_filt':v_avg_filt[ih+count_hr,:],\n",
    "               'w_avg_filt':w_avg_filt[ih+count_hr,:],\n",
    "               'T_avg_filt':T_avg_filt[ih+count_hr,:],\n",
    "               'u_std_ldtr':u_std_ldtr[ih+count_hr,:],\n",
    "               'v_std_ldtr':v_std_ldtr[ih+count_hr,:],\n",
    "               'w_std_ldtr':w_std_ldtr[ih+count_hr,:],\n",
    "               'T_std_ldtr':T_std_ldtr[ih+count_hr,:],\n",
    "               'u_std_filt':u_std_filt[ih+count_hr,:],\n",
    "               'v_std_filt':v_std_filt[ih+count_hr,:],\n",
    "               'w_std_filt':w_std_filt[ih+count_hr,:],\n",
    "               'T_std_filt':T_std_filt[ih+count_hr,:],\n",
    "               'uw_ldtr':uw_ldtr[ih+count_hr,:],\n",
    "               'vw_ldtr':vw_ldtr[ih+count_hr,:],\n",
    "               'wT_ldtr':wT_ldtr[ih+count_hr,:],\n",
    "               'u_star_ldtr':u_star_ldtr[ih+count_hr,:],\n",
    "               'H_ldtr':H_ldtr[ih+count_hr,:],\n",
    "               'uw_filt':uw_filt[ih+count_hr,:],\n",
    "               'vw_filt':vw_filt[ih+count_hr,:],\n",
    "               'wT_filt':wT_filt[ih+count_hr,:],\n",
    "               'u_star_filt':u_star_filt[ih+count_hr,:],\n",
    "               'H_filt':H_filt[ih+count_hr,:],\n",
    "               'u_star_ldtr':u_star_ldtr[ih+count_hr,:],\n",
    "               'u_star_filt':u_star_filt[ih+count_hr,:],\n",
    "               'L_H2_ldtr':L_H2_ldtr[ih+count_hr,:],\n",
    "               'stability_ldtr':stability_ldtr[ih+count_hr,:],\n",
    "               'L_H2_filt':L_H2_filt[ih+count_hr,:],\n",
    "               'stability_filt':stability_filt[ih+count_hr,:]\n",
    "               })\n",
    "        # print('end df')\n",
    "        # save tur and statistical data by hours\n",
    "        if write_results:   \n",
    "            # write output by hours\n",
    "            for var_name in out_tur:\n",
    "                var_value = globals()[var_name]\n",
    "                filename = f\"{var_name}_{strday}_{ih:02}00.npy\"\n",
    "                np.save(OUT_DIR + filename, var_value)\n",
    "            # save dataframe by hours as .csv files\n",
    "            dfname = f\"{'basic_statistics'}_{strday}_{ih:02}00.csv\"\n",
    "            df.to_csv(OUT_DIR + dfname, index=False)\n",
    "        # end of the hour loop\n",
    "    #end of the day loop\n",
    "    count_hr += 24\n",
    "# Done and save results\n",
    "if write_results: # write output by days\n",
    "    for var_name in out_avg+out_std+out_flux+out_stability:\n",
    "        var_value = globals()[var_name]\n",
    "        filename = f\"{var_name}.npy\"\n",
    "        np.save(OUT_DIR + filename, var_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e082cc8-2299-45d6-b738-3c5c24c662ff",
   "metadata": {},
   "source": [
    "# Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01234be9-11e6-4101-a042-23abd2aaed77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_statistical_data_092024/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a884578-b50a-4214-838f-e3fc26b8268e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0     11.201016\n",
       " 1     12.851772\n",
       " 2     14.985092\n",
       " 3     16.581162\n",
       " 4     18.074254\n",
       " 5     19.877515\n",
       " 6     20.444682\n",
       " 7     22.016416\n",
       " 8     22.549294\n",
       " 9     23.318245\n",
       " 10    23.578300\n",
       " 11    25.030723\n",
       " Name: u_avg_filt, dtype: float64,\n",
       " 0     11.204163\n",
       " 1     12.851771\n",
       " 2     14.988614\n",
       " 3     16.581032\n",
       " 4     18.085432\n",
       " 5     19.877515\n",
       " 6     20.445700\n",
       " 7     22.019322\n",
       " 8     22.571357\n",
       " 9     23.362946\n",
       " 10    23.595304\n",
       " 11    25.050651\n",
       " Name: u_avg_filt, dtype: float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfname = f\"{'basic_statistics'}_20210410_1600.csv\"\n",
    "OUT_DIR2 = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_statistical_data_091424/\"\n",
    "stat1 = pd.read_csv(OUT_DIR+dfname)\n",
    "stat2= pd.read_csv(OUT_DIR2+dfname)\n",
    "stat1['u_avg_filt'],stat2['u_avg_filt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a585b52-bf6e-4af3-90f9-7e6f0ac1639f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
