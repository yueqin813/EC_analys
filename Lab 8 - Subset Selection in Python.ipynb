{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab on Subset Selection is a Python adaptation of p. 244-247 of \"Introduction to Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. Adapted by R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5.1 Best Subset Selection\n",
    "\n",
    "Here we apply the best subset selection approach to the Hitters data. We\n",
    "wish to predict a baseball playerâ€™s Salary on the basis of various statistics\n",
    "associated with performance in the previous year. Let's take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hitters_df = pd.read_csv('Hitters.csv')\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we note that the `Salary` variable is missing for some of the\n",
    "players. The `isnull()` function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a `TRUE` value\n",
    "for any elements that are missing, and a `FALSE` value for non-missing elements.\n",
    "The `sum()` function can then be used to count all of the missing elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of null values:\", hitters_df[\"Salary\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `Salary` is missing for 59 players. The `dropna()` function\n",
    "removes all of the rows that have missing values in any variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the dimensions of the original Hitters data (322 rows x 20 columns)\n",
    "print(\"Dimensions of original data:\", hitters_df.shape)\n",
    "\n",
    "# Drop any rows the contain missing values, along with the player names\n",
    "hitters_df_clean = hitters_df.dropna().drop('Player', axis=1)\n",
    "\n",
    "# Print the dimensions of the modified Hitters data (263 rows x 20 columns)\n",
    "print(\"Dimensions of modified data:\", hitters_df_clean.shape)\n",
    "\n",
    "# One last check: should return 0\n",
    "print(\"Number of null values:\", hitters_df_clean[\"Salary\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(hitters_df_clean[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "y = hitters_df_clean.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = hitters_df_clean.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform best subset selection by identifying the best model that contains a given number of predictors, where **best** is quantified using RSS. We'll define a helper function to outputs the best set of variables for\n",
    "each model size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getBest(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a `DataFrame` containing the best model that we generated, along with some extra information about the model. Now we want to call that function for each number of predictors $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Could take quite awhile to complete...\n",
    "\n",
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,8):\n",
    "    models_best.loc[i] = getBest(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have one big `DataFrame` that contains the best models we've generated along with their RSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to access the details of each model, no problem! We can get a full rundown of a single model using the `summary()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(models_best.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output indicates that the best two-variable model\n",
    "contains only `Hits` and `CRBI`. To save time, we only generated results\n",
    "up to the best 7-variable model. You can use the functions we defined above to explore as many variables as are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Show the best 19-variable model (there's actually only one)\n",
    "print(getBest(19)[\"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than letting the results of our call to the `summary()` function print to the screen, we can access just the parts we need using the model's attributes. For example, if we want the $R^2$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_best.loc[2, \"model\"].rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! In addition to the verbose output we get when we print the summary to the screen, fitting the `OLM` also produced many other useful statistics such as adjusted $R^2$, AIC, and BIC. We can examine these to try to select the best overall model. Let's start by looking at $R^2$ across all our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Gets the second element from each row ('model') and pulls out its rsquared attribute\n",
    "models_best.apply(lambda row: row[1].rsquared, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the $R^2$ statistic increases monotonically as more\n",
    "variables are included.\n",
    "\n",
    "Plotting RSS, adjusted $R^2$, AIC, and BIC for all of the models at once will\n",
    "help us decide which model to select. Note the `type=\"l\"` option tells `R` to\n",
    "connect the plotted points with lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "# Set up a 2x2 grid so we can look at 4 plots at once\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "plt.plot(models_best[\"RSS\"])\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RSS')\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "rsquared_adj = models_best.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rsquared_adj)\n",
    "plt.plot(rsquared_adj.argmax(), rsquared_adj.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('adjusted rsquared')\n",
    "\n",
    "# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "aic = models_best.apply(lambda row: row[1].aic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin(), aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "bic = models_best.apply(lambda row: row[1].bic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin(), bic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the second step of our selection process, we narrowed the field down to just one model on any $k<=p$ predictors. We see that according to BIC, the best performer is the model with 6 variables. According to AIC and adjusted $R^2$ something a bit more complex might be better. Again, no one measure is going to give us an entirely accurate picture... but they all agree that a model with 5 or fewer predictors is insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5.2 Forward and Backward Stepwise Selection\n",
    "We can also use a similar approach to perform forward stepwise\n",
    "or backward stepwise selection, using a slight modification of the functions we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward(predictors):\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X.columns if p not in predictors]\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p]))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how much faster it runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_fwd = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,len(X.columns)+1):    \n",
    "    models_fwd.loc[i] = forward(predictors)\n",
    "    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! That's a lot better. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(models_fwd.loc[1, \"model\"].summary())\n",
    "print(models_fwd.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using forward stepwise selection, the best one-variable\n",
    "model contains only `Hits`, and the best two-variable model additionally\n",
    "includes `CRBI`. Let's see how the models stack up against best subset selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(models_best.loc[6, \"model\"].summary())\n",
    "print(models_fwd.loc[6, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, the best one-variable through six-variable\n",
    "models are each identical for best subset and forward selection.\n",
    "\n",
    "# Backward Selection\n",
    "Not much has to change to implement backward selection... just looping through the predictors in reverse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def backward(predictors):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)-1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_bwd = pd.DataFrame(columns=[\"RSS\", \"model\"], index = range(1,len(X.columns)))\n",
    "\n",
    "tic = time.time()\n",
    "predictors = X.columns\n",
    "\n",
    "while(len(predictors) > 1):  \n",
    "    models_bwd.loc[len(predictors)-1] = backward(predictors)\n",
    "    predictors = models_bwd.loc[len(predictors)-1][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, the best one-variable through six-variable\n",
    "models are each identical for best subset and forward selection.\n",
    "However, the best seven-variable models identified by forward stepwise selection,\n",
    "backward stepwise selection, and best subset selection are different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"------------\")\n",
    "print(\"Best Subset:\")\n",
    "print(\"------------\")\n",
    "print(models_best.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"-----------------\")\n",
    "print(\"Foward Selection:\")\n",
    "print(\"-----------------\")\n",
    "print(models_fwd.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"-------------------\")\n",
    "print(\"Backward Selection:\")\n",
    "print(\"-------------------\")\n",
    "print(models_bwd.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting credit\n",
    "To get credit for this lab, please post an example where you would choose to use each of the following:\n",
    "- Best subset\n",
    "- Forward selection\n",
    "- Backward selection\n",
    "\n",
    "to [Moodle](https://moodle.smith.edu/mod/quiz/view.php?id=257886)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
