{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14a79aa-29a0-4738-b1ef-989746b335dc",
   "metadata": {},
   "source": [
    "# 'Data_processing.ipynb' is created by Yue on Apr 26, 2023 for data preprocessing.\n",
    "\n",
    "Workflow:\n",
    "1. Load original instant data of ux,uy,uz.\n",
    "2. Filter by missing counts. Add flag when the number of nan exceeds 10%.\n",
    "4. Despike. Add flags when the number of spikes exceeds 1%.\n",
    "5. Calculate wind angle. Add flags when the wind angle is within (120,240) degrees.\n",
    "6. Tilt rotation.\n",
    "7. Detrend (linear detrending and high-pass filtering).\n",
    "8. Density correction. (neglect)\n",
    "9. Spectral correction. (neglect)\n",
    "\n",
    "Notes:\n",
    "1. input directory:/save_preprocessed_data.\n",
    "0. output directory: /save_processed_data.\n",
    "0. q,P measurements are very weird so I didn't perform de-spiking on them.\n",
    "0. qc=0: data is good; =1: data is bad.\n",
    "0. webb_corr=1: correct q,T,C; =2: correct q,C; =0: no correction.\n",
    "0. Do high-pass filtering on u,v,w,T with the same cutoff wavelength.\n",
    "0. All the instantaneous variables will be saved in separate files by hours.\n",
    "0. The hourly variables below will be saved in by days in: /save_processed_data.\\\n",
    "    \\['ts_dspk_wind_ang', 'u_filt_size'],\n",
    "    ['u_nspikes', 'v_nspikes', 'w_nspikes', 'T_nspikes'],\n",
    "    ['qc_ux_nan', 'qc_uy_nan', 'qc_uz_nan', 'qc_T_nan', 'qc_q_nan', 'qc_P_nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee5de0-50ad-48c4-a750-5304928d7e1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de32405f-c9b7-4a3a-95bf-54b240dc8e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This jupyter notebook command inserts matplotlib graphics in to the workbook\n",
    "%matplotlib inline\n",
    "\n",
    "# import packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "from matplotlib.pyplot import figure\n",
    "import scipy.io as sio\n",
    "from datetime import date, timedelta\n",
    "from math import *\n",
    "from scipy.stats import gmean\n",
    "from scipy import ndimage\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "from scipy.signal import butter,sosfiltfilt,filtfilt\n",
    "from scipy import fftpack\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3e231-fa3b-4287-a626-9fda3fd4a8a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e61de0b-cec2-4173-bdb3-d820c3555e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directories\n",
    "IN_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_preprocessed_data/\"\n",
    "OUT_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_processed_data/\"\n",
    "\n",
    "# physical constants (or values that are assumed constant)\n",
    "Rw  = 461.5     # ideal gas constant for water vapor, J/kg*K\n",
    "Rd  = 287.05    # ideal gas constant for dry air, J/kg*K\n",
    "Lv  = 1000*2257 # latent heat of vaporization (water), J/kg\n",
    "Cp  = 1005      # approximate constant pressure specific heat of air, J/kg*K\n",
    "k   = 0.4      # Von Karman constant\n",
    "g   = 9.81      # acceleration of gravity, m/s^2\n",
    "\n",
    "# global constants\n",
    "sonum    =12                       # number of sonic\n",
    "z  = np.array([1.2,2,3.5,6,9,12.5,16.5,23,30,40,50,60])  # height of sonic above ground, \n",
    "frequency=10                   # sampling rate, Hz\n",
    "time_avg =3600                  # average time, s\n",
    "rpat = time_avg*frequency           # number of lines for a loop\n",
    "\n",
    "# set the range of wind angle accepted\n",
    "min_wnd  = 120\n",
    "max_wnd  = 240\n",
    "\n",
    "# Filter requirements.\n",
    "T = time_avg         # Sample Period\n",
    "l_cutoff = 2000      # cutoff wavelength, m\n",
    "order = 10       # filter order\n",
    "nyq = 0.5 * frequency  # Nyquist Frequency\n",
    "n = int(T * frequency) # total number of samples\n",
    "\n",
    "# input variables\n",
    "ins_var=['u_ins','v_ins','w_ins','Tsonic_ins','diag_csat_ins','q_ins','P_ins']\n",
    "\n",
    "# output variables\n",
    "out_tur = ['ux_dspk', 'uy_dspk', 'uz_dspk', 'T_dspk', 'q_ins_rnan', 'P_ins_rnan',\n",
    "           'u_dspk_2rot_ldtr', 'v_dspk_2rot_ldtr', 'w_dspk_2rot_ldtr', 'T_dspk_ldtr', \n",
    "           'u_dspk_2rot_trend', 'v_dspk_2rot_trend', 'w_dspk_2rot_trend', 'T_dspk_trend',\n",
    "           'u_dspk_2rot_filt', 'v_dspk_2rot_filt', 'w_dspk_2rot_filt', 'T_dspk_filt']\n",
    "out_other = ['ts_dspk_wind_ang', 'u_filt_size']\n",
    "out_qf = ['qc_ux_nan', 'qc_uy_nan', 'qc_uz_nan', 'qc_T_nan','qc_q_nan', 'qc_P_nan',\n",
    "          'qc_ux_dspk', 'qc_uy_dspk', 'qc_uz_dspk', 'qc_T_dspk','qc_wdir_dspk']\n",
    "out_nspikes = ['u_nspikes', 'v_nspikes', 'w_nspikes', 'T_nspikes']\n",
    "\n",
    "# controls\n",
    "# webb_corr = 2 # do webb-corr on q and C only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf678d-9839-42d2-bf16-18d132d57c22",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6592bfe4-587b-49d7-a32e-29d74fbc7d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def date_list(sdate,edate):\n",
    "    \"\"\"method used for creating date list\"\"\"\n",
    "    delta = edate - sdate       # as timedelta\n",
    "    day = [sdate+timedelta(days=x) for x in range(delta.days+1)]\n",
    "    return day\n",
    "\n",
    "def rmnan(data,flag):  \n",
    "    \"\"\" method used for checking & rm nan\"\"\"  \n",
    "    data[flag >= 65] = np.nan ## ！！！！！ get rid of bad data (thershold is 65)! ! ! ! !\n",
    "    nansum = np.sum(np.isnan(data))\n",
    "    qc = 0\n",
    "    if nansum >= time_avg*frequency/10:\n",
    "        data[:] = np.nan\n",
    "        qc = 1\n",
    "    return data,qc\n",
    "    # for icol in range(sonum):\n",
    "    #     nansum = np.sum(np.isnan(data[:,icol]),0)\n",
    "    #     # if there are more than 10% nan in an hour, discard all data at that level\n",
    "    #     if nansum >= time_avg*frequency/10:\n",
    "    #         data[:,icol] = np.nan\n",
    "    # return data\n",
    "def get_wind_ang(ux,uy,nl):\n",
    "    \"\"\"\n",
    "    !!!arctan return values in radians!!!\n",
    "    for 1 level only\n",
    "    calculate mean wind angle on the xy-plane (!! must do before double rotation)\n",
    "    The CSAT3 (the anemometer arms, tripods) is aligned northward and if u is positive, the wind is northerly. \n",
    "    If v is positive, the wind is westerly.\n",
    "    u:north(+)->south(-), v:west(+)->east(-)\n",
    "    \"\"\"\n",
    "    u_avg = np.nanmean(ux,axis=0) # size = 1\n",
    "    v_avg = np.nanmean(uy,axis=0)\n",
    "    rot_ang_v = degrees(np.arctan(v_avg/u_avg))\n",
    "    # print('nl='+str(nl))\n",
    "    # if nl==2:\n",
    "    #     wind_ang = rot_ang_v*np.nan\n",
    "    #     mask1 = np.logical_and(u_avg>=0, v_avg<=0)\n",
    "    #     wind_ang[mask1] = -rot_ang_v[mask1] # northeast\n",
    "    #     mask2 = np.logical_and(u_avg<=0, v_avg<=0)\n",
    "    #     wind_ang[mask2] = 180-rot_ang_v[mask2] # southest\n",
    "    #     mask3 = np.logical_and(u_avg<=0, v_avg>=0)\n",
    "    #     wind_ang[mask3] = 180-rot_ang_v[mask3] # southwest\n",
    "    #     mask4 = np.logical_and(u_avg>=0, v_avg>=0)\n",
    "    #     wind_ang[mask4] = 360-rot_ang_v[mask4] # northwest         \n",
    "    if nl==1:\n",
    "        wind_ang = rot_ang_v\n",
    "        if np.logical_and(u_avg>=0, v_avg<=0):\n",
    "            wind_ang = -rot_ang_v # northeast\n",
    "        if np.logical_and(u_avg<=0, v_avg<=0):\n",
    "            wind_ang = 180-rot_ang_v # southest\n",
    "        if np.logical_and(u_avg<=0, v_avg>=0):\n",
    "            wind_ang = 180-rot_ang_v # southwest\n",
    "        if np.logical_and(u_avg>=0, v_avg>=0):\n",
    "            wind_ang = 360-rot_ang_v # northwest \n",
    "    return wind_ang\n",
    "\n",
    "def wind_ang(ux,uy):\n",
    "    \"\"\"\n",
    "    calculate mean wind angle on the xy-plane (!! must do before double rotation)\n",
    "    The CSAT3 (the anemometer arms, tripods) is aligned northward and if u is positive, the wind is northerly. \n",
    "    If v is positive, the wind is westerly.\n",
    "    u:north(+)->south(-), v:west(+)->east(-)\n",
    "    \"\"\"\n",
    "    u_avg = np.nanmean(ux,axis=0) # size = 12\n",
    "    v_avg = np.nanmean(uy,axis=0)\n",
    "    rot_ang_v = np.arctan(v_avg/u_avg)\n",
    "    rot_ang_v = rot_ang_v*360/2/math.pi\n",
    "    mask1 = np.logical_and(u_avg>=0, v_avg<=0)\n",
    "    rot_ang_v[mask1] = -rot_ang_v[mask1] # northeast\n",
    "    mask2 = np.logical_and(u_avg<=0, v_avg<=0)\n",
    "    rot_ang_v[mask2] = 180-rot_ang_v[mask2] # southest\n",
    "    mask3 = np.logical_and(u_avg<=0, v_avg>=0)\n",
    "    rot_ang_v[mask3] = 180-rot_ang_v[mask3] # southwest\n",
    "    mask4 = np.logical_and(u_avg>=0, v_avg>=0)\n",
    "    rot_ang_v[mask4] = 360-rot_ang_v[mask4] # northwest\n",
    "    \n",
    "    # quality control: wind angle>120 and < 240\n",
    "    qc = np.zeros(12)\n",
    "    msk = np.logical_and(rot_ang_v> min_wnd,rot_ang_v<max_wnd)\n",
    "    qc[msk] = 1\n",
    "    return rot_ang_v,qc\n",
    "\n",
    "def double_rot(ux,uy,uz):\n",
    "    \"\"\"\n",
    "    Double rotation method (Note yaw correction must perform before pitch correction)\n",
    "    https://www.licor.com/env/support/EddyPro/topics/anemometer-tilt-correction.html#:~:\n",
    "    text=Double%20rotation%20method,by%20the%20flux%20averaging%20length.\n",
    "    # such that the hourly avg of v and w will be zero\n",
    "    # only u,v,w will be rotated, other variables remain the same\n",
    "    \"\"\"\n",
    "    u_avg = np.nanmean(ux,axis=0) # size = 12\n",
    "    v_avg = np.nanmean(uy,axis=0)\n",
    "    w_avg = np.nanmean(uz,axis=0)\n",
    "    # 1) yaw rotation (to remove v component)\n",
    "    C1 = (u_avg**2 + v_avg**2) ** 0.5 # Magnitude of horizontal wind vector\n",
    "    rot_mat_1 = u_avg/C1 # cos(theta)\n",
    "    rot_mat_2 = v_avg/C1 # sin(theta)\n",
    "    rot_mat_3 = -v_avg/C1\n",
    "    rot_mat_4 = u_avg/C1\n",
    "    u_rot = ux * rot_mat_1 + uy*rot_mat_2\n",
    "    v_rot = ux * rot_mat_3 + uy*rot_mat_4\n",
    "    u_ins_yawrot = u_rot # Intermediate rotated u after yaw correction\n",
    "    v_ins_2rot = v_rot\n",
    "    u_avg_yawrot = np.nanmean(u_ins_yawrot,axis=0) # Recompute u mean after yaw rotation\n",
    "    # v_avg_2rot = np.nanmean(v_ins_2rot,axis=0)\n",
    "\n",
    "    # 2) pitch rotation (to remove w component)\n",
    "    C2 = (u_avg_yawrot**2 + w_avg**2) ** 0.5 # Magnitude of u-w plane vector\n",
    "    rot_mat_1 = u_avg_yawrot/C2\n",
    "    rot_mat_2 = w_avg/C2  # cos(phi)\n",
    "    rot_mat_3 =-w_avg/C2  # sin(phi)\n",
    "    rot_mat_4 = u_avg_yawrot/C2\n",
    "    u_rot = u_ins_yawrot * rot_mat_1 + uz*rot_mat_2\n",
    "    w_rot = u_ins_yawrot * rot_mat_3 + uz*rot_mat_4\n",
    "    u_ins_2rot = u_rot\n",
    "    w_ins_2rot = w_rot\n",
    "    # u_avg_2rot = np.nanmean(u_ins_2rot,axis=0)\n",
    "    # w_avg_2rot = np.nanmean(w_ins_2rot,axis=0)\n",
    "    \n",
    "    return u_ins_2rot,v_ins_2rot,w_ins_2rot\n",
    "\n",
    "def butter_lowpass_filter(filt_type, data, cutoff, fs, order):\n",
    "    \"\"\"\n",
    "    The frequency response of the Butterworth filter is maximally flat \n",
    "    (i.e. has no ripples) in the passband and rolls off towards zero in the stopband, \n",
    "    hence its one of the most popular low pass filter.\n",
    "    \n",
    "    data shoule be turbulent component!\n",
    "    \"\"\"\n",
    "    # replace nan by mean value\n",
    "    data[np.argwhere(np.isnan(data))] = np.nanmean(data)\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    sos = butter(order, normal_cutoff, btype=filt_type, analog=False, output='sos',fs=fs)\n",
    "    # Return the filtered output with the same shape as data\n",
    "    # The function sosfiltfilt should be preferred over filtfilt \n",
    "    # for most filtering tasks, as second-order sections have fewer numerical problems.\n",
    "    y = sosfiltfilt(sos, data) \n",
    "    return y\n",
    "\n",
    "def get_u_star(u_tur_in,v_tur_in,w_tur_in):\n",
    "    uw = np.nanmean(u_tur_in*w_tur_in,axis=0)\n",
    "    vw = np.nanmean(v_tur_in*w_tur_in,axis=0)\n",
    "    u_star = (np.maximum(0,(uw**2+vw**2)**0.5))**0.5\n",
    "    return u_star\n",
    "  \n",
    "\n",
    "def dtrd(data):\n",
    "    # Only return the turbulent component  \n",
    "    # The result is equal to signal.detrend(data)+np.mean(data)\n",
    "    ct = np.arange(len(data))\n",
    "    a = -(len(data)*np.nansum(ct*data, axis=0) - np.nansum(ct, axis=0) * \n",
    "          np.nansum(data, axis=0)) / (np.nansum(ct**2, axis=0)-(np.nansum(ct, axis=0))**2)\n",
    "    b = (np.nansum(data, axis=0) - a * np.nansum(ct, axis=0))/len(data)\n",
    "    data_dtr = data + (a*ct+b) - np.nanmean(data)\n",
    "    return data_dtr\n",
    "\n",
    "def get_ist(data):\n",
    "    \"\"\"\n",
    "    Return the non-stationarity index for every hourly time series \n",
    "    \"\"\"\n",
    "    data_5min = data.reshape([12,-1]) # split each hour into 12 chunks/every 5 min\n",
    "    cvm = np.nanmean(np.nanvar(data_5min,axis=1)) # avg of the variance of each chunck\n",
    "    ist_5min = abs(cvm-np.nanvar(data))/np.nanvar(data)\n",
    "    return ist_5min\n",
    "\n",
    "def z_score(intensity):\n",
    "    \"\"\"\n",
    "    Z-score based approach for spike detection\n",
    "    \"\"\"\n",
    "    mean_int = np.nanmean(intensity)\n",
    "    std_int = np.nanstd(intensity)\n",
    "    z_scores = (intensity-mean_int) / std_int\n",
    "    return z_scores\n",
    "\n",
    "def fixer_backup(y,thres):\n",
    "    \"\"\"\n",
    "    remove spikes and fix them with the mean of its immediate neighbors.\n",
    "    Following Vickers and Mahrt (1997), \n",
    "    https://www.licor.com/env/support/EddyPro/topics/despiking-raw-statistical-screening.html\n",
    "    \"\"\"\n",
    "    qc = 0\n",
    "    it = 0\n",
    "    y_original = y.copy()\n",
    "    while it < 20: # iterate 20 times\n",
    "        # print(f\"the {it} iterations:\")\n",
    "        # print(f\"the 0 windows\")\n",
    "        y_sub = y[0:12000] # moving window is 20 min\n",
    "        y_fix = y_sub.copy()\n",
    "        spikes = abs(np.array(z_score(y_sub))) > thres\n",
    "        n_con_spk = 0 # counts of more than 4 consecutive spikes\n",
    "        for i in np.where(spikes != 0)[0]:  # If we have an spike in position i\n",
    "            if i == 12000 - 1:\n",
    "                w2 = np.arange(i-3,i+1)\n",
    "                w1 = w2\n",
    "            elif i == 12000 - 2:\n",
    "                w2 = np.arange(i-4,i)\n",
    "                w1 = np.arange(i-3,i+1)\n",
    "            elif i == 12000 - 3:\n",
    "                w2 = np.arange(i-5,i-1)\n",
    "                w1 = np.arange(i-3,i+1)\n",
    "            else:\n",
    "                w1 = np.arange(i-3,i+1)\n",
    "                w2 = np.arange(i,i+4)\n",
    "            # 4 consecutive outliers\n",
    "            # are considered as a local trend and not counted as a spike. \n",
    "            if np.sum(spikes[w1])==4 or np.sum(spikes[w2])==4:\n",
    "                # print(f\"4 consecutive spikes.\")\n",
    "                n_con_spk += 1\n",
    "            else:\n",
    "                if i == 12000 - 1:\n",
    "                    w = np.arange(12000-3,12000)\n",
    "                else:\n",
    "                    w = np.arange(i-1,i+2) # we select 3 points around our spike\n",
    "                ww = w[spikes[w] == 0] # From such interval, we choose the ones which are not spikes\n",
    "                y_fix[i] = np.mean(y_sub[ww]) # and we average their values\n",
    "        # nspikes = np.nansum(spikes) - n_con_spk\n",
    "        y_new = y_fix\n",
    "        # 2nd to 6th moving window\n",
    "        for iw in np.arange(1,5):\n",
    "            n_con_spk = 0\n",
    "            # print(f\"the {iw} windows\")\n",
    "            y_sub = y[iw*6000:iw*6000+12000]\n",
    "            y_fix = y_sub.copy()\n",
    "            spikes = abs(np.array(z_score(y_sub))) > thres\n",
    "            for i in np.where(spikes != 0)[0]:  # If we have an spike in position i\n",
    "                if i >= 6000:\n",
    "                    if i == 12000 - 1:\n",
    "                        w2 = np.arange(i-3,i+1)\n",
    "                        w1 = w2\n",
    "                    elif i == 12000 - 2:\n",
    "                        w2 = np.arange(i-4,i)\n",
    "                        w1 = np.arange(i-3,i+1)\n",
    "                    elif i == 12000 - 3:\n",
    "                        w2 = np.arange(i-5,i-1)\n",
    "                        w1 = np.arange(i-3,i+1)\n",
    "                    else:\n",
    "                        w1 = np.arange(i-3,i+1)\n",
    "                        w2 = np.arange(i,i+4)\n",
    "                    # 4 consecutive outliers\n",
    "                    # are considered as a local trend and not counted as a spike. \n",
    "                    if np.sum(spikes[w1])==4 or np.sum(spikes[w2])==4:         \n",
    "                        # print(f\"4 consecutive spikes.\")\n",
    "                        n_con_spk += 1\n",
    "                    else:\n",
    "                        if i == 12000 - 1:\n",
    "                            w = np.arange(12000-3,12000)\n",
    "                        else:\n",
    "                            w = np.arange(i-1,i+2) # we select 3 points around our spike\n",
    "                        ww = w[spikes[w] == 0] # From such interval, we choose the ones which are not spikes\n",
    "                        y_fix[i] = np.mean(y_sub[ww]) # and we average their values\n",
    "            nspk = np.nansum(spikes[6000:12000]) - n_con_spk\n",
    "            y_new = np.append(y_new,y_fix[6000:12000])\n",
    "            # nspikes += nspk\n",
    "        # print(f\"{nspikes} spikes\")\n",
    "        # print(\"-----------\")        \n",
    "        # if it == 0:\n",
    "        #     n_spikes = nspikes\n",
    "        # else:\n",
    "        n_spikes = np.sum((y_new-y_original)!=0)\n",
    "        if n_spikes > 0.01*rpat: # accepted spikes is 1%\n",
    "            qc = 1 # quality flag = 1, should be discarded from the results dataset.\n",
    "            print(\"Too much number of spikes\")\n",
    "            break\n",
    "        if n_spikes == 0:\n",
    "            break\n",
    "        y = y_new\n",
    "        it += 1\n",
    "    return y_new,qc,n_spikes\n",
    "\n",
    "def fixer(y,thres):\n",
    "    \"\"\"\n",
    "    remove spikes and fix them with the mean of its immediate neighbors.\n",
    "    Following Vickers and Mahrt (1997), \n",
    "    https://www.licor.com/env/support/EddyPro/topics/despiking-raw-statistical-screening.html\n",
    "    \"\"\"\n",
    "    qc = 0\n",
    "    it = 0\n",
    "    y_original = y.copy()\n",
    "    while it < 20: # iterate 20 times\n",
    "        # print(f\"the {it} iterations:\")\n",
    "        # print(f\"the 0 windows\")\n",
    "        y_sub = y[0:12000] # moving window is 20 min\n",
    "        y_fix = y_sub.copy()\n",
    "        spikes = abs(np.array(z_score(y_sub))) > thres\n",
    "        n_con_spk = 0 # counts of more than 4 consecutive spikes\n",
    "        for i in np.where(spikes)[0]:  # If we have an spike in position i\n",
    "            if i == 12000 - 1:\n",
    "                w2 = np.arange(i-3,i+1)\n",
    "                w1 = w2\n",
    "            elif i == 12000 - 2:\n",
    "                w2 = np.arange(i-4,i)\n",
    "                w1 = np.arange(i-3,i+1)\n",
    "            elif i == 12000 - 3:\n",
    "                w2 = np.arange(i-5,i-1)\n",
    "                w1 = np.arange(i-3,i+1)\n",
    "            else:\n",
    "                w1 = np.arange(i-3,i+1)\n",
    "                w2 = np.arange(i,i+4)\n",
    "            # 4 consecutive outliers\n",
    "            # are considered as a local trend and not counted as a spike. \n",
    "            if np.sum(spikes[w1])==4 or np.sum(spikes[w2])==4:\n",
    "                # print(f\"4 consecutive spikes.\")\n",
    "                n_con_spk += 1\n",
    "            else:\n",
    "                if i == 12000 - 1:\n",
    "                    w = np.arange(12000-3,12000)\n",
    "                else:\n",
    "                    w = np.arange(i-1,i+2) # we select 3 points around our spike\n",
    "                ww = w[spikes[w] == 0] # From such interval, we choose the ones which are not spikes\n",
    "                y_fix[i] = np.mean(y_sub[ww]) # and we average their values\n",
    "        # nspikes = np.nansum(spikes) - n_con_spk\n",
    "        y_new = y_fix\n",
    "        # 2nd to 6th moving window\n",
    "        for iw in np.arange(1,5):\n",
    "            n_con_spk = 0\n",
    "            # print(f\"the {iw} windows\")\n",
    "            y_sub = y[iw*6000:iw*6000+12000]\n",
    "            y_fix = y_sub.copy()\n",
    "            spikes = abs(np.array(z_score(y_sub))) > thres\n",
    "            for i in np.where(spikes)[0]:  # If we have an spike in position i\n",
    "                if i >= 6000:\n",
    "                    if i == 12000 - 1:\n",
    "                        w2 = np.arange(i-3,i+1)\n",
    "                        w1 = w2\n",
    "                    elif i == 12000 - 2:\n",
    "                        w2 = np.arange(i-4,i)\n",
    "                        w1 = np.arange(i-3,i+1)\n",
    "                    elif i == 12000 - 3:\n",
    "                        w2 = np.arange(i-5,i-1)\n",
    "                        w1 = np.arange(i-3,i+1)\n",
    "                    else:\n",
    "                        w1 = np.arange(i-3,i+1)\n",
    "                        w2 = np.arange(i,i+4)\n",
    "                    # 4 consecutive outliers\n",
    "                    # are considered as a local trend and not counted as a spike. \n",
    "                    if np.sum(spikes[w1])==4 or np.sum(spikes[w2])==4:         \n",
    "                        # print(f\"4 consecutive spikes.\")\n",
    "                        n_con_spk += 1\n",
    "                    else:\n",
    "                        if i == 12000 - 1:\n",
    "                            w = np.arange(12000-3,12000)\n",
    "                        else:\n",
    "                            w = np.arange(i-1,i+2) # we select 3 points around our spike\n",
    "                        ww = w[spikes[w] == 0] # From such interval, we choose the ones which are not spikes\n",
    "                        y_fix[i] = np.mean(y_sub[ww]) # and we average their values\n",
    "            nspk = np.nansum(spikes[6000:12000]) - n_con_spk\n",
    "            y_new = np.append(y_new,y_fix[6000:12000])\n",
    "            # nspikes += nspk\n",
    "        # print(f\"{nspikes} spikes\")\n",
    "        # print(\"-----------\")        \n",
    "        # if it == 0:\n",
    "        #     n_spikes = nspikes\n",
    "        # else:\n",
    "        n_spikes = np.sum((y_new-y_original)!=0)\n",
    "        if n_spikes > 0.01*rpat: # accepted spikes is 1%\n",
    "            qc = 1 # quality flag = 1, should be discarded from the results dataset.\n",
    "            print(\"Too much number of spikes\")\n",
    "            break\n",
    "        if n_spikes == 0:\n",
    "            break\n",
    "        y = y_new\n",
    "        it += 1\n",
    "    return y_new,qc,n_spikes\n",
    "\n",
    "\n",
    "def abs_lim(y,lim):\n",
    "    \"\"\"\n",
    "    After de-spiking,\n",
    "    replace a value that is outside a user-defined plausible range \n",
    "    by the mean of neiboring variables. \n",
    "    \"\"\"\n",
    "    out_lier = abs(y)>lim\n",
    "    # print(np.sum(out_lier))\n",
    "    y_lim = y.copy()\n",
    "    for i in np.where(out_lier!=0)[0]:\n",
    "        w = np.arange(i-2,i+3) # select 5 points around \n",
    "        w2 = w[out_lier[w] == 0] # From such interval, we choose the ones which are not outliers\n",
    "        y_lim[i] = np.mean(y[w2]) # and we average their values\n",
    "    return y_lim\n",
    "\n",
    "def CheckForLess(list1, val): \n",
    "    # traverse in the list\n",
    "    for x in list1: \n",
    "        # compare with all the\n",
    "        # values with value\n",
    "        if val <= x:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d48163-845d-4736-b8ef-d3836f8dcf75",
   "metadata": {},
   "source": [
    "# Load data and do the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e2d844-4e5b-4c4c-948b-6f0b329f5205",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_processed_data/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up time period and initialize variables\n",
    "Sdate = date(2020,9,25)\n",
    "# Sdate = date(2020,10,17)\n",
    "Edate = date(2021,4,23)\n",
    "# Edate = date(2020,9,26)\n",
    "ds = date_list(Sdate,Edate)\n",
    "write_results = 1\n",
    "OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca274291-6e17-4fd7-9309-c6dbcd5592b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for day in ds:\n",
    "    strday = str(day.strftime(\"%Y%m%d\"))\n",
    "    fp_stats = IN_DIR + 'u_ins_' + strday +'.pkl'\n",
    "    if (not os.path.isfile(fp_stats)):\n",
    "        print(day.strftime(\"%Y%m%d\")+' do not exist')\n",
    "        continue\n",
    "    print('start processing:'+ strday)\n",
    "    # load data    \n",
    "    for var in ins_var:\n",
    "    # for var in ['q_ins','P_ins','diag_csat_ins']:\n",
    "        a_file = open(IN_DIR + var + '_' + strday +'.pkl', \"rb\")\n",
    "        globals()[var] = pickle.load(a_file)\n",
    "        a_file.close()\n",
    "    u_ins3d = u_ins.reshape(-1,rpat,sonum)\n",
    "    v_ins3d = v_ins.reshape(-1,rpat,sonum)\n",
    "    w_ins3d = w_ins.reshape(-1,rpat,sonum)\n",
    "    T_ins3d = Tsonic_ins.reshape(-1,rpat,sonum)\n",
    "    q_ins3d = q_ins.reshape(-1,rpat,sonum)\n",
    "    P_ins3d = P_ins.reshape(-1,rpat,sonum)\n",
    "    diag_ins3d = diag_csat_ins.reshape(-1,rpat,sonum)\n",
    "  \n",
    "    # initialize avg data every day --------------------------\n",
    "    ## initialize qaulity flag to be 0\n",
    "    qc_ux_nan = np.zeros((24,sonum)) \n",
    "    qc_uy_nan = np.zeros((24,sonum))\n",
    "    qc_uz_nan = np.zeros((24,sonum))\n",
    "    qc_T_nan = np.zeros((24,sonum))\n",
    "    qc_q_nan = np.zeros((24,sonum))\n",
    "    qc_P_nan = np.zeros((24,sonum))\n",
    "    \n",
    "    qc_ux_dspk = np.zeros((24,sonum)) \n",
    "    qc_uy_dspk = np.zeros((24,sonum))\n",
    "    qc_uz_dspk = np.zeros((24,sonum))\n",
    "    qc_T_dspk = np.zeros((24,sonum))\n",
    "    \n",
    "    ## number of spikes\n",
    "    u_nspikes = np.zeros((24,sonum)) * np.nan\n",
    "    v_nspikes = np.zeros((24,sonum)) * np.nan\n",
    "    w_nspikes = np.zeros((24,sonum)) * np.nan\n",
    "    T_nspikes = np.zeros((24,sonum)) * np.nan\n",
    "    \n",
    "    qc_wdir_dspk = np.zeros((24,sonum))\n",
    "    \n",
    "    ## hourly averaged wind angle\n",
    "    ts_dspk_wind_ang = np.zeros((24,sonum)) * np.nan \n",
    "    ## high-pass filter size\n",
    "    u_filt_size = np.zeros((24,sonum)) * np.nan  \n",
    "    \n",
    "    ##-------------------------------------------------\n",
    "    # start the loop over hours\n",
    "    for ih in range(24): \n",
    "        ux_ts = u_ins3d[ih,:,:] #36000*12\n",
    "        uy_ts = v_ins3d[ih,:,:]\n",
    "        uz_ts = w_ins3d[ih,:,:]\n",
    "        T_ts = T_ins3d[ih,:,:]\n",
    "        q_ts = q_ins3d[ih,:,:]\n",
    "        P_ts = P_ins3d[ih,:,:]\n",
    "        diag = diag_ins3d[ih,:,:]\n",
    "        \n",
    "        #initialize tur data every hour\n",
    "        ux_dspk = np.zeros((rpat,sonum)) * np.nan\n",
    "        uy_dspk = np.zeros((rpat,sonum)) * np.nan\n",
    "        uz_dspk = np.zeros((rpat,sonum)) * np.nan\n",
    "        T_dspk = np.zeros((rpat,sonum)) * np.nan\n",
    "\n",
    "        u_dspk_2rot_ldtr = np.zeros((rpat,sonum)) * np.nan\n",
    "        v_dspk_2rot_ldtr = np.zeros((rpat,sonum)) * np.nan\n",
    "        w_dspk_2rot_ldtr = np.zeros((rpat,sonum)) * np.nan\n",
    "        T_dspk_ldtr = np.zeros((rpat,sonum)) * np.nan\n",
    "\n",
    "        u_dspk_2rot_trend = np.zeros((rpat,sonum)) * np.nan\n",
    "        v_dspk_2rot_trend = np.zeros((rpat,sonum)) * np.nan\n",
    "        w_dspk_2rot_trend = np.zeros((rpat,sonum)) * np.nan\n",
    "        T_dspk_trend = np.zeros((rpat,sonum)) * np.nan\n",
    "\n",
    "        u_dspk_2rot_filt = np.zeros((rpat,sonum)) * np.nan\n",
    "        v_dspk_2rot_filt = np.zeros((rpat,sonum)) * np.nan\n",
    "        w_dspk_2rot_filt = np.zeros((rpat,sonum)) * np.nan\n",
    "        T_dspk_filt = np.zeros((rpat,sonum)) * np.nan\n",
    "        q_ins_rnan = np.zeros((rpat,sonum)) * np.nan\n",
    "        P_ins_rnan = np.zeros((rpat,sonum)) * np.nan\n",
    "    \n",
    "        # Step 1: remove nan\n",
    "        # start_time = time.time()\n",
    "        # replace the whole chunk by nan if nan exceeds 10% in an hour\n",
    "        # Get rid of bad data with diag_csat >= 65\n",
    "        for il in range(12): # loop over levels\n",
    "            ux_ts[:,il],qc_ux_nan[ih,il] = rmnan(ux_ts[:,il],diag[:,il])\n",
    "            uy_ts[:,il],qc_uy_nan[ih,il] = rmnan(uy_ts[:,il],diag[:,il])\n",
    "            uz_ts[:,il],qc_uz_nan[ih,il] = rmnan(uz_ts[:,il],diag[:,il])\n",
    "            T_ts[:,il],qc_T_nan[ih,il] = rmnan(T_ts[:,il],diag[:,il])\n",
    "            q_ins_rnan[:,il],qc_q_nan[ih,il] = rmnan(q_ts[:,il],diag[:,il])\n",
    "            P_ins_rnan[:,il],qc_P_nan[ih,il] = rmnan(P_ts[:,il],diag[:,il])\n",
    "        # print('rmnan done')\n",
    "        \n",
    "        # Step 2: De-spiking\n",
    "        # start_time = time.time()\n",
    "        for il in range(12): # loop over levels\n",
    "            # if whole chunck is nan then skip de-spiking\n",
    "            if np.any([qc_ux_nan[ih,il],qc_uy_nan[ih,il],qc_uz_nan[ih,il],qc_T_nan[ih,il]]):\n",
    "                continue\n",
    "            else:\n",
    "                ux_dspk[:,il],qc_ux_dspk[ih,il],u_nspikes[ih,il] = fixer(ux_ts[:,il],thres=3.5) # 36000*12, 24*12, 24*12\n",
    "                uy_dspk[:,il],qc_uy_dspk[ih,il],v_nspikes[ih,il] = fixer(uy_ts[:,il],thres=3.5)\n",
    "                uz_dspk[:,il],qc_uz_dspk[ih,il],w_nspikes[ih,il] = fixer(uz_ts[:,il],thres=5)\n",
    "                T_dspk[:,il],qc_T_dspk[ih,il],T_nspikes[ih,il] = fixer(T_ts[:,il],thres=3.5)\n",
    "        # print('despike done')  \n",
    "        # end_time = time.time()\n",
    "        # duration = end_time - start_time\n",
    "        # print(f\"Time taken for dspk: {duration} seconds\")\n",
    "        \n",
    "        # Step 3: Calculte wind angle on the xy-plane \n",
    "        for il in range(12):\n",
    "            ts_dspk_wind_ang[ih,il] = get_wind_ang(ux_dspk[:,il],uy_dspk[:,il],1)\n",
    "            if np.logical_and(ts_dspk_wind_ang[ih,il]> min_wnd,ts_dspk_wind_ang[ih,il]<max_wnd):\n",
    "                qc_wdir_dspk[ih,il] = 1\n",
    "        # ts_dspk_wind_ang[ih,:],qc_wdir_dspk[ih,:] = wind_ang(ux_dspk[ih,:,:],uy_dspk[ih,:,:]) # 1*12, 1*12\n",
    "        \n",
    "        # Step 4: Double rotation\n",
    "        # start_time = time.time()\n",
    "        u_dspk_2rot,v_dspk_2rot,w_dspk_2rot = double_rot(ux_dspk,uy_dspk,uz_dspk)\n",
    "        # print('2rot done')\n",
    "        # end_time = time.time()\n",
    "        # duration = end_time - start_time\n",
    "        # print(f\"Time taken for double rotation: {duration} seconds\")\n",
    "        \n",
    "        # Calculate mean variables\n",
    "        u_avg_dspk_2rot = np.nanmean(u_dspk_2rot,axis=0) # 12*1\n",
    "        v_avg_dspk_2rot = np.nanmean(v_dspk_2rot,axis=0)\n",
    "        w_avg_dspk_2rot = np.nanmean(w_dspk_2rot,axis=0)\n",
    "        T_avg_dspk = np.nanmean(T_dspk[:,:],axis=0)\n",
    "          \n",
    "        # Step 5: Detrend\n",
    "        # start_time = time.time()\n",
    "        for il in range(12): # loop over levels\n",
    "            # if whole chunck is nan then skip detrend\n",
    "            if np.any([qc_ux_nan[ih,il],qc_uy_nan[ih,il],qc_uz_nan[ih,il],qc_T_nan[ih,il]]):\n",
    "                continue\n",
    "            else:\n",
    "                ## 4.1 Linear detrend\n",
    "                u_dspk_2rot_ldtr[:,il] = dtrd(u_dspk_2rot[:,il])\n",
    "                v_dspk_2rot_ldtr[:,il] = dtrd(v_dspk_2rot[:,il])\n",
    "                w_dspk_2rot_ldtr[:,il] = dtrd(w_dspk_2rot[:,il])\n",
    "                T_dspk_ldtr[:,il] = dtrd(T_dspk[:,il])\n",
    "\n",
    "                ## 4.2 high-pass filter detrend\n",
    "                u_cutoff = u_avg_dspk_2rot[il]/l_cutoff     # desired cutoff frequency of the filter, Hz\n",
    "                # print(cutoff)\n",
    "                u_filt_size[ih,il] = int(1/u_cutoff) # seconds\n",
    "                # print(filt_size)\n",
    "                u_tur = u_dspk_2rot[:,il]-u_avg_dspk_2rot[il]\n",
    "                u_dspk_2rot_trend[:,il] = butter_lowpass_filter('low', u_tur, u_cutoff, frequency, order)\n",
    "                u_dspk_2rot_filt[:,il] = u_dspk_2rot[:,il]-u_dspk_2rot_trend[:,il]\n",
    "                # print('1')\n",
    "                \n",
    "                v_tur = v_dspk_2rot[:,il]-v_avg_dspk_2rot[il]\n",
    "                v_dspk_2rot_trend[:,il] = butter_lowpass_filter('low', v_tur, u_cutoff, frequency, order)\n",
    "                v_dspk_2rot_filt[:,il] = v_dspk_2rot[:,il]-v_dspk_2rot_trend[:,il]\n",
    "                # print('2')\n",
    "                \n",
    "                w_tur = w_dspk_2rot[:,il]-w_avg_dspk_2rot[il]\n",
    "                w_dspk_2rot_trend[:,il] = butter_lowpass_filter('low', w_tur, u_cutoff, frequency, order)\n",
    "                w_dspk_2rot_filt[:,il] = w_dspk_2rot[:,il]-w_dspk_2rot_trend[:,il]\n",
    "                # print('3')\n",
    "                \n",
    "                T_tur = T_dspk[:,il]-T_avg_dspk[il]\n",
    "                T_dspk_trend[:,il] = butter_lowpass_filter('low', T_tur, u_cutoff, frequency, order)\n",
    "                T_dspk_filt[:,il] = T_dspk[:,il]-T_dspk_trend[:,il]\n",
    "                \n",
    "        # end_time = time.time()\n",
    "        # duration = end_time - start_time\n",
    "        # print(f\"Time taken for detrending: {duration} seconds\")  \n",
    "        # print('detrend done') \n",
    "        \n",
    "        ### end of the hour loop\n",
    "        if write_results:\n",
    "            # print(f\"start writing {ih}hour\")    \n",
    "            for var_name in out_tur: # write output by hours\n",
    "            # for var_name in ['q_ins_rnan','P_ins_rnan']: # write output by hours\n",
    "                # Access the variable by name using globals() - this allows you to get the variable's value by its name as a string\n",
    "                var_value = globals()[var_name]\n",
    "                # Construct the filename using the variable's name and the specified date, then save the array to a .npy file\n",
    "                filename = f\"{var_name}_{strday}_{ih:02}00.npy\"\n",
    "                np.save(OUT_DIR + filename, var_value)\n",
    "    ## end of the day loop\n",
    "    if write_results: # write output by days\n",
    "        for var_name in out_other+out_qf+out_nspikes:\n",
    "        # for var_name in ['qc_q_nan','qc_P_nan']:\n",
    "            var_value = globals()[var_name]\n",
    "            filename = f\"{var_name}_{strday}.npy\"\n",
    "            np.save(OUT_DIR + filename, var_value)\n",
    "    \n",
    "    # print('finish:'+ strday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e35872-492e-41f7-b8aa-e7ba66507cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%whos ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b4f8d-b7b4-4031-8604-15b6f895c1f3",
   "metadata": {},
   "source": [
    "# Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45fdd32-9f4e-43d7-b4c3-d4b819c657ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/processed_data_020724/ts_dspk_wind_ang_20200925.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m day1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20200925\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m dir1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/processed_data_020724/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m a_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdir1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mts_dspk_wind_ang_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mday1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m u_std_filt1 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(a_file)\n\u001b[1;32m      5\u001b[0m a_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/projectnb/urbanclimate/yueqin/.conda/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/processed_data_020724/ts_dspk_wind_ang_20200925.pkl'"
     ]
    }
   ],
   "source": [
    "day1 = '20200925'\n",
    "dir1 = '/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/processed_data_020724/'\n",
    "a_file = open(dir1 + 'ts_dspk_wind_ang_' + day1 +'.pkl', \"rb\")\n",
    "u_std_filt1 = pickle.load(a_file)\n",
    "a_file.close()\n",
    "\n",
    "u_std_filt2 = np.load(OUT_DIR + 'ts_dspk_wind_ang_' + day1 +'.npy')\n",
    "u_std_filt1-u_std_filt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad003ae8-a1e2-43d5-bea2-a682dc2709fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ins_var:\n",
    "    # for var in ['q_ins','P_ins','diag_csat_ins']:\n",
    "    a_file = open(IN_DIR + var + '_20210423.pkl', \"rb\")\n",
    "    globals()[var] = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "u_ins3d = u_ins.reshape(-1,rpat,sonum)\n",
    "v_ins3d = v_ins.reshape(-1,rpat,sonum)\n",
    "w_ins3d = w_ins.reshape(-1,rpat,sonum)\n",
    "T_ins3d = Tsonic_ins.reshape(-1,rpat,sonum)\n",
    "q_ins3d = q_ins.reshape(-1,rpat,sonum)\n",
    "P_ins3d = P_ins.reshape(-1,rpat,sonum)\n",
    "diag_ins3d = diag_csat_ins.reshape(-1,rpat,sonum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72e121-e3ad-455f-9ee3-1f848b0b2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ins3d[-9,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98396ea7-becf-4fc8-a330-76f685793b8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example: plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525b781-d346-469c-867b-0509b124355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4), dpi=150,tight_layout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f574e-48b9-4907-98fd-56f4f3ed2476",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4), dpi=150,tight_layout=True)\n",
    "ax1 = fig.add_subplot(121) \n",
    "plt.plot(u_std_filt[0,list_sel_m2])\n",
    "# plt.ylim(0,1.2)\n",
    "# plt.plot(u_star_ldtr)\n",
    "ax2 = fig.add_subplot(122) \n",
    "plt.plot(u_std_ldtr[0,list_sel_m2])\n",
    "# plt.ylim(0,1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf70e7-7d72-46bb-9843-93914f3fb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4), dpi=150,tight_layout=True)\n",
    "plt.plot(L_H2_filt[:,1])\n",
    "plt.plot(L_H2_ldtr[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725d59e-e933-415d-a21e-49cb59c62085",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4), dpi=150,tight_layout=True)\n",
    "# plt.plot(u_2rot_dspk_filt[:,0])\n",
    "plt.plot(u_dspk_2rot_ldtr[:,0])\n",
    "plt.plot(u_dspk_2rot_filt[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
