{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fec3c4f-ea34-4253-b4b3-616d20b4e459",
   "metadata": {},
   "source": [
    "# 'Fit_statistics.ipynb' is created by Yue on Feb 12, 2024 for computing specific statistics.\n",
    "\n",
    "Workflow:\n",
    "1. Load basic statistics, masks and datetime.\n",
    "2. Fit the u_avg profile.\n",
    "3. Determine ISL.\n",
    "4. Fit the u_std profile.\n",
    "5. Fit w_std against u_star.\n",
    "6. Compute IST for u, wdir\n",
    "7. Compute index for ISL and wind shear\n",
    "\n",
    "Notes:\n",
    "1. input directory: /save_mask_data, /save_statistical_data.\n",
    "2. output directory: /save_fitted_data.\n",
    "3. Some mean wind profiles can have negative slope, which will not produce z_0.\n",
    "4. datetime_all files contain hours from 0 to 23.\n",
    "5. Results from fitting u_avg profile are saved in f\"{level}_u_avg_fitting_1Ddata.pkl\" and \"f{level}_u_avg_fitting_2Ddata.pkl\". Different ranges of u_avg are used to determine the ISL.\n",
    "6. After deciding to use 5th to 11th levels as ISL, results are saved in \"u_std_fitting_1Ddata.pkl\" and \"u_std_fitting_2Ddata.pkl\".\n",
    "7. index_shear has a shape of [n_hours,sonum-1]\n",
    "\n",
    "=========== Disable de-spike on Sep 14, 2024 and save data to /save_fitted_data_091424 =====\n",
    "\n",
    "=========== Disable de-spike and do planar fit on Sep 19, 2024 and save data to /save_fitted_data_planarfit =====\n",
    "\n",
    "=========== Disable de-spike and do planar fit on Sep 19, 2024 and save data to /save_fitted_data_092024 =====\n",
    "\n",
    "*** Add azimuth correction for each sensor on Oct 1, 2024 \n",
    "\n",
    "=========== Disable de-spike and do double rotation on Oct 1, 2024 and save data to /save_processed_data_2rot ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2d0d7-3417-4fd3-92d4-5356c172b733",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c691301-6398-48ba-8023-52a4566e8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import time\n",
    "from matplotlib.pyplot import figure\n",
    "import scipy.io as sio\n",
    "from datetime import date, timedelta\n",
    "from math import *\n",
    "from scipy.stats import gmean\n",
    "from scipy import ndimage\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "import matplotlib.transforms as mtransforms\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a4672-4fc7-493e-9ff1-ef573682016c",
   "metadata": {},
   "source": [
    "# Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fc304-448f-4fcd-bad1-13327528ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "IN_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_statistical_data_091424/\"\n",
    "IN_MSK_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_mask_data_091424/\"\n",
    "IN_INS_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_processed_data_091424/\"\n",
    "\n",
    "OUT_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_fitted_data_091424/\"\n",
    "OUT_PLOT_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_fig_2rot/\"\n",
    "\n",
    "# List of directories to check\n",
    "directories = [IN_DIR, IN_MSK_DIR, IN_INS_DIR, OUT_DIR, OUT_PLOT_DIR]\n",
    "\n",
    "# Check if directories exist, create them if they don't\n",
    "for dir_path in directories:\n",
    "    if not os.path.exists(dir_path):\n",
    "        try:\n",
    "            os.makedirs(dir_path)\n",
    "            print(f\"Created directory: {dir_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating directory {dir_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Directory exists: {dir_path}\")\n",
    "\n",
    "# labels\n",
    "list_bot = np.array([0,1,2,3,4]) # bottom five levels\n",
    "list_all = np.arange(0, 12, 1)\n",
    "list_sel = np.array([5,6,7,8,9,10]) # from level 6 to level 11\n",
    "list_test1 = np.array([4,5,6,7,8,9,10,11])\n",
    "list_test2 = np.array([4,5,6,7,8,9,10])\n",
    "list_test3 = np.array([5,6,7,8,9,10,11])\n",
    "\n",
    "# global constants\n",
    "sonum    =12                       # number of sonic\n",
    "z  = np.array([1.2,2,3.5,6,9,12.5,16.5,23,30,40,50,60])  # height of sonic above ground, \n",
    "frequency=10                   # sampling rate, Hz\n",
    "time_avg =3600                  # average time, s\n",
    "\n",
    "# physical constants (or values that are assumed constant)\n",
    "Rw  = 461.5     # ideal gas constant for water vapor, J/kg*K\n",
    "Rd  = 287.05    # ideal gas constant for dry air, J/kg*K\n",
    "Lv  = 1000*2257 # latent heat of vaporization (water), J/kg\n",
    "Cp  = 1005      # approximate constant pressure specific heat of air, J/kg*K\n",
    "kappa_assumed = 0.4 # von karman constant\n",
    "g   = 9.81      # acceleration of gravity, m/s^2\n",
    "nu = 1.48 * 10**(-5) # kinematic viscosity, m2 /s\n",
    "\n",
    "\n",
    "# plotting\n",
    "font_size = 10\n",
    "mark_size = 5\n",
    "plt.rc('text', usetex=True)\n",
    "# plt.rc('font', family='sans-serif')\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rc('xtick', labelsize=font_size) \n",
    "plt.rc('ytick', labelsize=font_size)\n",
    "# label physical distance in and down:\n",
    "# trans = mtransforms.ScaledTranslation(-20/72, 7/72, fig.dpi_scale_trans)\n",
    "\n",
    "# input variables\n",
    "in_avg = ['wind_ang_all','u_avg_ldtr', 'u_avg_filt']\n",
    "in_std = ['u_std_ldtr', 'u_std_filt','w_std_ldtr', 'w_std_filt']\n",
    "in_flux = ['u_star_ldtr', 'H_ldtr', 'u_star_filt', 'H_filt']\n",
    "in_stability = ['L_H2_ldtr', 'stability_ldtr', 'L_H2_filt', 'stability_filt']\n",
    "in_mask = ['mask_rnan', 'mask_dspk', 'mask_neutral', 'mask_wdir', 'mask_taylor',\n",
    "           'mask_ustar_gt005', 'mask_H_gt10']\n",
    "in_tur = ['u_dspk_2rot_ldtr', 'u_dspk_2rot_filt', 'ux_dspk', 'uy_dspk']\n",
    "# Define detrending types\n",
    "var_types = ['ldtr', 'filt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f17fd-c2dc-4cfc-a99d-a2eb3f68cf16",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0618da1-d66c-43c8-b9dc-6625c7c255f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ist_wspd_10min(data):\n",
    "    \"\"\"\n",
    "    Return the non-stationarity index for every hourly time series \n",
    "    \"\"\"\n",
    "    data_10min = data.reshape([6,-1]) # split each hour into 6 chunks/every 10 min\n",
    "    cvm = np.nanmean(np.nanvar(data_10min,axis=1)) # avg of the variance of each chunck\n",
    "    # print(np.nanvar(data_10min,axis=1))\n",
    "    # print(cvm)\n",
    "    ist_10min = abs(cvm-np.nanvar(data))/np.nanvar(data)\n",
    "    return ist_10min\n",
    "\n",
    "def ist_wspd_5min(data):\n",
    "    \"\"\"\n",
    "    Return the non-stationarity index for every hourly time series \n",
    "    \"\"\"\n",
    "    data_5min = data.reshape([12,-1]) # split each hour into 12 chunks/every 5 min\n",
    "    cvm = np.nanmean(np.nanvar(data_5min,axis=1)) # avg of the variance of each chunck\n",
    "    # print(np.nanvar(data_10min,axis=1))\n",
    "    # print(cvm)\n",
    "    ist_5min = abs(cvm-np.nanvar(data))/np.nanvar(data)\n",
    "    return ist_5min\n",
    "\n",
    "def convert_ang(wdir,nl):\n",
    "    \"\"\"\n",
    "    Convert wind direction from [0,360] to [-180,180]\n",
    "    nl=2->2Darray; nl=1->1Darray, nl=0->0D\n",
    "    \"\"\"\n",
    "    if nl==2:\n",
    "        wdir_new = wdir.copy()\n",
    "        wdir_new[wdir>180] = wdir_new[wdir>180] - 360\n",
    "        wdir_new[wdir<-180] = wdir_new[wdir<-180] + 360\n",
    "    if nl==1:\n",
    "        wdir_new = wdir.copy()\n",
    "        for i in range(len(wdir)):\n",
    "            if wdir[i]>180:\n",
    "                wdir_new[i] = wdir[i] - 360\n",
    "            if wdir[i]<-180:\n",
    "                wdir_new[i] = wdir[i] + 360\n",
    "            # else:\n",
    "            #     wdir_new[i] = wdir[i]\n",
    "    if nl==0:\n",
    "        if wdir>180:\n",
    "            wdir_new = wdir - 360\n",
    "        if wdir<-180:\n",
    "            wdir_new = wdir + 360\n",
    "        else:\n",
    "            wdir_new = wdir\n",
    "    return wdir_new\n",
    "\n",
    "def get_wind_ang(ux,uy,nl):\n",
    "    \"\"\"\n",
    "    For 1 level only\n",
    "    Calculate mean wind angle on the xy-plane (!! must do before double rotation)\n",
    "    The CSAT3 (the anemometer arms, tripods) is aligned northward and if u is positive, the wind is northerly. \n",
    "    If v is positive, the wind is westerly.\n",
    "    u:north(+)->south(-), v:west(+)->east(-)\n",
    "    \"\"\"\n",
    "    u_avg = np.nanmean(ux,axis=0) # size = 1\n",
    "    v_avg = np.nanmean(uy,axis=0)\n",
    "    rot_ang_v = degrees(np.arctan(v_avg/u_avg))      \n",
    "    if nl==1:\n",
    "        wind_ang = rot_ang_v\n",
    "        if np.logical_and(u_avg>=0, v_avg<=0):\n",
    "            wind_ang = -rot_ang_v # northeast\n",
    "        if np.logical_and(u_avg<=0, v_avg<=0):\n",
    "            wind_ang = 180-rot_ang_v # southest\n",
    "        if np.logical_and(u_avg<=0, v_avg>=0):\n",
    "            wind_ang = 180-rot_ang_v # southwest\n",
    "        if np.logical_and(u_avg>=0, v_avg>=0):\n",
    "            wind_ang = 360-rot_ang_v # northwest \n",
    "    return wind_ang\n",
    "\n",
    "def ist_wdir(u_ins,v_ins,wind_ang_1hr):\n",
    "    \"\"\"\n",
    "    Calculate IST(wdir) = |wdir5min-wdir1hr|max\n",
    "    \"\"\"\n",
    "    u_ins_5min = u_ins.reshape([12,-1]) # split each hour into 12 chunks/every 5 min\n",
    "    v_ins_5min = v_ins.reshape([12,-1]) # split each hour into 12 chunks/every 5 min\n",
    "    # calculate 5 min wind angle\n",
    "    wind_ang_temp = np.zeros(12)*np.nan\n",
    "    # print(wind_ang_1hr)\n",
    "    for i in range(12): # 12 chunks\n",
    "        # print(i)\n",
    "        wind_ang_temp[i] = get_wind_ang(u_ins_5min[i,:],v_ins_5min[i,:],1)\n",
    "    # print('before-----------------')\n",
    "    # print(wind_ang_temp)\n",
    "    wind_ang_5min = convert_ang(wind_ang_temp,1) # convert wind direction to [-180,180]  \n",
    "    # print('after-----------------')\n",
    "    # print(wind_ang_5min)\n",
    "    diff_temp = abs(wind_ang_5min-wind_ang_1hr)\n",
    "    wdir_diff_max = np.nanmax(diff_temp)\n",
    "    return wdir_diff_max\n",
    "\n",
    "def get_uavg_fit(_z,_uavg):\n",
    "    \"\"\"\n",
    "    Compute u_star or kappa by fitting the mean wind profile\n",
    "    note that it might have some wierd profiles\n",
    "    e.g. negative slope -> can't calculate z0\n",
    "    \n",
    "    \"\"\"\n",
    "    mask1 = ~np.isnan(_z)\n",
    "    mask2 = ~np.isnan(_uavg)\n",
    "    mask = np.logical_and(mask1,mask2) # mask out nan values\n",
    "    slope_,intercept_,r_,_,_ = stats.linregress(np.log(_z[mask]), _uavg[mask])\n",
    "    r2_ = r_**2 # r-square value\n",
    "    # print(slope_,intercept_,r_)\n",
    "    # print(-intercept_/slope_)\n",
    "    if slope_ <= 0.1 :\n",
    "        # print('negative slope in u_avg profile')\n",
    "        return slope_,intercept_,r2_,np.nan\n",
    "    else:\n",
    "        # print(intercept_,slope_)\n",
    "        z0_ = exp(-intercept_/slope_) # momentum roughness length           \n",
    "        return slope_,intercept_,r2_,z0_\n",
    "\n",
    "def get_ustd_fit_backup(_z,_y):\n",
    "    \"\"\"\n",
    "    Compute Townsend's coefficient A1 by fitting the streamwise velocity variance.\n",
    "    Because I use -log(z) to fit agianst u_std^2/u_star^2, the slope is positive and is equal to A1_fit.\n",
    "    \"\"\"\n",
    "    mask1 = ~np.isnan(_z)\n",
    "    mask2 = ~np.isnan(_y)\n",
    "    mask = np.logical_and(mask1,mask2) # mask out nan values\n",
    "    rsl = stats.linregress(-np.log(_z[mask]), _y[mask])\n",
    "    A1_ = rsl.slope\n",
    "    r2_A1_ = rsl.rvalue**2\n",
    "    return A1_,r2_A1_\n",
    "\n",
    "def get_ustd_fit(_z,_y):\n",
    "    \"\"\"\n",
    "    Compute Townsend's coefficient A1 by fitting the streamwise velocity variance.\n",
    "    Because I use -log(z) to fit agianst u_std^2/u_star^2, the slope is positive and is equal to A1_fit.\n",
    "    \"\"\"\n",
    "    mask1 = ~np.isnan(_z)\n",
    "    mask2 = ~np.isnan(_y)\n",
    "    mask = np.logical_and(mask1,mask2) # mask out nan values\n",
    "    rsl = stats.linregress(np.log(_z[mask]), _y[mask])\n",
    "    slope_ = rsl.slope\n",
    "    r2_ = rsl.rvalue**2\n",
    "    intercept_ = rsl.intercept\n",
    "    return slope_, intercept_, r2_\n",
    "\n",
    "def calculate_u_star_means_and_uavg_fits(levels, types, index):\n",
    "    for level in levels:\n",
    "        if level == 'all_level':\n",
    "            range_ = list_all\n",
    "        elif level == 'sel_level':\n",
    "            range_ = list_sel\n",
    "        elif level == 'test1':\n",
    "            range_ = list_test1\n",
    "        elif level == 'test2':\n",
    "            range_ = list_test2\n",
    "        elif level == 'test3':\n",
    "            range_ = list_test3\n",
    "            \n",
    "        for type_ in types:\n",
    "            # Calculate u_star mean\n",
    "            if type_ == 'ldtr':\n",
    "                var1d[level][type_]['u_star_mean'][index] = np.nanmean(u_star_ldtr[index, range_])\n",
    "            elif type_ == 'filt':\n",
    "                var1d[level][type_]['u_star_mean'][index] = np.nanmean(u_star_filt[index, range_])\n",
    "            \n",
    "            # Update fits\n",
    "            (var1d[level][type_]['slope_uavg'][index], \n",
    "             var1d[level][type_]['intercept_uavg'][index], \n",
    "             var1d[level][type_]['r2_uavg'][index], \n",
    "             var1d[level][type_]['z0_fit'][index]) = get_uavg_fit(z[range_], globals()[f'u_avg_{type_}'][index, range_])\n",
    "            \n",
    "            # Calculate kappa_fit\n",
    "            var1d[level][type_]['kappa_fit'][index] = var1d[level][type_]['u_star_mean'][index] / var1d[level][type_]['slope_uavg'][index]\n",
    "            \n",
    "            # Calculate u_star_fit\n",
    "            var1d[level][type_]['u_star_fit'][index] = kappa_assumed * var1d[level][type_]['slope_uavg'][index]\n",
    "\n",
    "def calculate_u_star_vertical_diff(levels, types, index):\n",
    "    \"\"\" \n",
    "    u_star_dev = (u_star-u_star_mean)/u_star_mean\n",
    "    u_star_diff = (u_star-u_star_fit)/u_star_fit\n",
    "    \"\"\"\n",
    "    for level in levels:\n",
    "        for type_ in types:\n",
    "            if type_ == 'ldtr':\n",
    "                var2d[level][type_]['u_star_dev'][index,:] = (u_star_ldtr[index,:] - var1d[level][type_]['u_star_mean'][index])/var1d[level][type_]['u_star_mean'][index]\n",
    "                var2d[level][type_]['u_star_diff'][index,:] = (u_star_ldtr[index,:] - var1d[level][type_]['u_star_fit'][index])/var1d[level][type_]['u_star_fit'][index]\n",
    "            elif type_ == 'filt':\n",
    "                var2d[level][type_]['u_star_dev'][index,:] = (u_star_filt[index,:] - var1d[level][type_]['u_star_mean'][index])/var1d[level][type_]['u_star_mean'][index]\n",
    "                var2d[level][type_]['u_star_diff'][index,:] = (u_star_filt[index,:] - var1d[level][type_]['u_star_fit'][index])/var1d[level][type_]['u_star_fit'][index]\n",
    "\n",
    "def cal_stability_corrected_kappa(type_, index):\n",
    "    \"\"\"\n",
    "    Use Businger-Dyer equations to estimate the stability correction function\n",
    "    \"\"\"\n",
    "    if varISL_1d[type_]['stability_parameter'][index] > 0: # stable condition\n",
    "        varISL_1d[type_]['kappa_corrected'][index] = var1d['sel_level'][type_]['kappa_fit'][index]*(1+5*varISL_1d[type_]['stability_parameter'][index]) \n",
    "    else: # unstable condition\n",
    "        varISL_1d[type_]['kappa_corrected'][index] = var1d['sel_level'][type_]['kappa_fit'][index]*(1-16*varISL_1d[type_]['stability_parameter'][index])**(-1/4)\n",
    "\n",
    "def cal_ustd_fits_backup(type_, index):\n",
    "    # Normalize u_std by u_star_mean\n",
    "    y_u_std = globals()[f'u_std_{type_}'][index,list_sel]**2 / (var1d['sel_level'][type_]['u_star_mean'][index]**2)\n",
    "    # Update fits\n",
    "    varISL_1d[type_]['A1_fit'][index],varISL_1d[type_]['r2_ustd'][index] = get_ustd_fit(z[list_sel],y_u_std)\n",
    "    \n",
    "def cal_ustd_fits(type_, index):\n",
    "    # Normalize u_std by u_star_mean\n",
    "    y_u_std = globals()[f'u_std_{type_}'][index,list_sel]**2 / (var1d['sel_level'][type_]['u_star_mean'][index]**2)\n",
    "    # Update fits\n",
    "    varISL_1d[type_]['slope_ustd'][index],varISL_1d[type_]['intercept_ustd'][index],varISL_1d[type_]['r2_ustd'][index] = get_ustd_fit(z[list_sel],y_u_std)\n",
    "    varISL_1d[type_]['A1_fit'][index] = -varISL_1d[type_]['slope_ustd'][index]\n",
    "\n",
    "# Function to calculate slope with intercept forced to 0\n",
    "# def cal_wstd_fits_through_origin(type_, index):\n",
    "#     y = globals()[f'w_std_{type_}'][index, list_sel]\n",
    "#     x = globals()[f'u_star_{type_}'][index, list_sel]\n",
    "    \n",
    "#     # Mask out nan values\n",
    "#     mask1 = ~np.isnan(x)\n",
    "#     mask2 = ~np.isnan(y)\n",
    "#     mask = np.logical_and(mask1, mask2)\n",
    "    \n",
    "#     # Filter x and y with the mask\n",
    "#     x_filtered = x[mask]\n",
    "#     y_filtered = y[mask]\n",
    "    \n",
    "#     # Reshape x_filtered to match the requirement of np.linalg.lstsq\n",
    "#     x_filtered = x_filtered.reshape(-1, 1)\n",
    "    \n",
    "#     # Perform linear regression with intercept forced to 0\n",
    "#     slope_, _, _, _ = np.linalg.lstsq(x_filtered, y_filtered, rcond=None)\n",
    "    \n",
    "#     # Calculate R-squared (coefficient of determination)\n",
    "#     y_pred = x_filtered * slope_\n",
    "#     ss_tot = np.sum((y_filtered - np.mean(y_filtered))**2)\n",
    "#     ss_res = np.sum((y_filtered - y_pred)**2)\n",
    "#     r2_ = 1 - (ss_res / ss_tot)\n",
    "#     print(r2_)\n",
    "\n",
    "#     # Store results\n",
    "#     varISL_1d[type_]['slope_wstd_ustar'][index] = slope_\n",
    "#     varISL_1d[type_]['r2_wstd'][index] = r2_\n",
    "\n",
    "def cal_wstd_fits_through_origin(type_, index):\n",
    "    \"\"\"\n",
    "    Fit the slope of w_std^2 over u_star^2 within ISL\n",
    "    \"\"\"\n",
    "    y = (globals()[f'w_std_{type_}'][index, list_sel])**2\n",
    "    x = (globals()[f'u_star_{type_}'][index, list_sel])**2\n",
    "    \n",
    "    # Mask out nan values\n",
    "    mask1 = ~np.isnan(x)\n",
    "    mask2 = ~np.isnan(y)\n",
    "    mask = np.logical_and(mask1, mask2)\n",
    "    \n",
    "    # Filter x and y with the mask\n",
    "    x_filtered = x[mask]\n",
    "    y_filtered = y[mask]\n",
    "    \n",
    "    # Perform linear regression with intercept forced to 0\n",
    "    slope_, _, _, _ = np.linalg.lstsq(x_filtered[:, np.newaxis], y_filtered, rcond=None)\n",
    "    \n",
    "    # Calculate predicted values\n",
    "    y_pred = x_filtered * slope_\n",
    "    \n",
    "    # Calculate the total sum of squares (ss_tot) and residual sum of squares (ss_res)\n",
    "    ss_tot = np.sum(y_filtered**2)  # Since regression is through the origin, use the sum of squares of y\n",
    "    ss_res = np.sum((y_filtered - y_pred)**2)\n",
    "    \n",
    "    # Calculate R^2 (coefficient of determination)\n",
    "    r2_ = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    # Store results\n",
    "    varISL_1d[type_]['slope_wstd_ustar'][index] = slope_[0]\n",
    "    varISL_1d[type_]['r2_wstd'][index] = r2_\n",
    "\n",
    "    print(f\"Slope: {slope_[0]}, R-squared: {r2_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021841f8-1891-4a17-91ea-45454b45057c",
   "metadata": {},
   "source": [
    "# load necessary variables\n",
    "also load n_hours, masks, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330db43-2718-459b-a210-7c03d9ee0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in in_avg+in_std+in_flux+in_stability:\n",
    "    globals()[var_name] = np.load(f\"{IN_DIR}{var_name}.npy\")\n",
    "for var_name in in_mask:\n",
    "    globals()[var_name] = np.load(f\"{IN_MSK_DIR}{var_name}.npy\")\n",
    "\n",
    "mask = mask_rnan*mask_dspk*mask_neutral*mask_wdir*mask_taylor*mask_ustar_gt005\n",
    "n_hours = np.load(f\"{IN_DIR}{'n_hours'}.npy\")\n",
    "# mask_rnan = np.load(f\"{IN_MSK_DIR}{'mask_rnan'}.npy\")\n",
    "datetime_all = np.load(f\"{IN_DIR}{'datetime_all'}.npy\", allow_pickle=True)\n",
    "write_results = True\n",
    "OUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba8f0aa-fd92-4274-94ec-f7ea066645c2",
   "metadata": {},
   "source": [
    "# Determine the inertial sublayer\n",
    "Fit u_avg with different vertical range and compare the fitted kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b7e12-67d8-4b8f-9c29-88d6008cf0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define variable names\n",
    "var1d_names = ['u_star_mean', 'kappa_fit', 'u_star_fit', 'slope_uavg', 'intercept_uavg', 'r2_uavg', 'z0_fit']\n",
    "var2d_names = ['u_star_dev','u_star_diff']\n",
    "\n",
    "# Define level names\n",
    "var1d_levels = ['all_level','sel_level','test1','test2','test3']\n",
    "var2d_levels = ['all_level','sel_level','test1','test2','test3']\n",
    "\n",
    "# Initialize all variables using nested dictionary comprehensions\n",
    "var1d = {level: {type_: {name: np.zeros(n_hours) * np.nan for name in var1d_names} \n",
    "               for type_ in var_types} for level in var1d_levels}\n",
    "# var2d_levels = ['all_level','sel_level']\n",
    "var2d = {level: {type_: {name: np.zeros((n_hours,sonum))*np.nan for name in var2d_names} \n",
    "               for type_ in var_types} for level in var2d_levels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1b7c1-1134-4f81-a14c-09a4e6714ede",
   "metadata": {},
   "source": [
    "# Calculate mean u_star and vertical deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323cc11-d34b-41b4-b0f2-f4180c47c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1bedce-58a2-4ac2-89a2-2223e273cb0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(n_hours):\n",
    "    if mask_rnan[i]: # skip nan hours\n",
    "        # Estimate u_star by vertically averaged\n",
    "        # Fit uavg profiles to get fitted kappa\n",
    "        calculate_u_star_means_and_uavg_fits(var1d_levels, var_types, i)\n",
    "        # Calculate vertical deviation\n",
    "        calculate_u_star_vertical_diff(var2d_levels, var_types, i)\n",
    "\n",
    "# Save results\n",
    "if write_results:\n",
    "    # save all levels together\n",
    "    filename = \"u_avg_fitting_1Ddata.pkl\"\n",
    "    with open(OUT_DIR + filename, 'wb') as f:\n",
    "        pickle.dump(var1d, f)\n",
    "    filename = \"u_avg_fitting_2Ddata.pkl\"\n",
    "    with open(OUT_DIR + filename, 'wb') as f:\n",
    "        pickle.dump(var2d, f)\n",
    "    # Separate dictionary by levels\n",
    "    for level in var1d_levels:\n",
    "        data_to_save = var1d[level]\n",
    "        filename = f\"{level}_u_avg_fitting_1Ddata.pkl\"\n",
    "        with open(OUT_DIR + filename, 'wb') as f:\n",
    "            pickle.dump(data_to_save, f)\n",
    "            \n",
    "    for level in var2d_levels:\n",
    "        data_to_save = var2d[level]\n",
    "        filename = f\"{level}_u_avg_fitting_2Ddata.pkl\"\n",
    "        with open(OUT_DIR + filename, 'wb') as f:\n",
    "            pickle.dump(data_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb1e72-1d85-4d7c-9b35-391d675e439e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d5cb0-4f5e-4027-a1d1-fcea1bd4684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"u_avg_fitting_1Ddata.pkl\"\n",
    "var1d = np.load(OUT_DIR + filename, allow_pickle=True)\n",
    "filename = \"u_avg_fitting_2Ddata.pkl\"\n",
    "var2d = np.load(OUT_DIR + filename, allow_pickle=True)\n",
    "\n",
    "IN_MSK_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_mask_data_091424/\"\n",
    "in_mask = ['mask_rnan', 'mask_dspk', 'mask_neutral', 'mask_wdir', 'mask_taylor',\n",
    "           'mask_ustar_gt005', 'mask_H_gt10']\n",
    "for var_name in in_mask:\n",
    "    globals()[var_name] = np.load(f\"{IN_MSK_DIR}{var_name}.npy\")\n",
    "    \n",
    "IN_STAT_DIR = \"/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/save_statistical_data_091424/\"    \n",
    "in_avg = ['u_avg_ldtr', 'u_avg_filt', 'Rho_air', 'P_avg', 'stability_filt']\n",
    "for var_name in in_avg:\n",
    "    globals()[var_name] = np.load(f\"{IN_STAT_DIR}{var_name}.npy\", allow_pickle=True)\n",
    "\n",
    "Rho_air_mean = np.nanmean(Rho_air[:,list_sel],axis=1)\n",
    "mask_rho = Rho_air_mean<1.25    \n",
    "mask1 = mask_rnan*mask_neutral*mask_wdir*mask_taylor*mask_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed878ffd-db43-474d-90b0-d9973d6850b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "var2d['all_level']['ldtr']['u_star_dev'].shape,mask1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da8180-2d81-409a-8e30-370382e70811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sum(mask1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e841774-d633-4763-a3ed-cd798dbd9e64",
   "metadata": {},
   "source": [
    "# Plot Vertical profile of the locally measured u*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9c82c-6085-4b21-a6f1-b392228a8b7f",
   "metadata": {},
   "source": [
    "## Linear detrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608a96f-eab3-4e82-9d71-20673a37f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88ada8-3491-4dc2-8e53-368086e8807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PLOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee09383-30e7-47a8-8b82-3d99dbf66977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(6, 5), dpi=150,tight_layout=True)\n",
    "text_loc = [-0.1, 0.98]\n",
    "fig, axes = plt.subplot_mosaic([['a','b']], constrained_layout=True)\n",
    "fig.set_size_inches(10, 10)\n",
    "fig.set_dpi(200)\n",
    "font_size = 12\n",
    "# label physical distance in and down:\n",
    "trans = mtransforms.ScaledTranslation(-20/72, 7/72, fig.dpi_scale_trans)\n",
    "medianprops = dict(linestyle='-', linewidth=1, color='k')\n",
    "sample = var2d['all_level']['ldtr']['u_star_dev'][mask1]\n",
    "mask = ~np.isnan(sample) # Filter data using np.isnan\n",
    "filtered_data = [d[m] for d, m in zip(sample.T, mask.T)]\n",
    "# filtered_data = np.array(filtered_data)\n",
    "axes['a'].boxplot(filtered_data[:12], positions=z, vert=False, widths=0.8,\n",
    "                medianprops=medianprops)\n",
    "# axes['a'].boxplot(filtered_data[[0,1,2,3,4,5,6,7,8,9,10,11]],positions=z, vert=False, widths=0.8,\n",
    "#                  medianprops=medianprops)\n",
    "axes['a'].set_xlim(-0.6,0.6)\n",
    "axes['a'].set_ylim(0,61)\n",
    "# plot reference lines\n",
    "axes['a'].axvline(x=0, color='grey', linestyle='--', linewidth=1)\n",
    "axes['a'].axvline(x=0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['a'].axvline(x=-0.25, color='grey', linestyle='--', linewidth=1)\n",
    "# axes['a'].axhspan(12, 50.5, facecolor='grey', alpha=0.25)\n",
    "\n",
    "axes['a'].set_yticks(z)\n",
    "axes['a'].tick_params(axis='y', labelsize=font_size+2)\n",
    "axes['a'].tick_params(axis='x', labelsize=font_size+2)\n",
    "axes['a'].set_ylabel(r'$\\it z$ (m)', fontsize=font_size+2)\n",
    "axes['a'].set_xlabel(r'$\\it [u_*(z)-\\left<u^{all}_*\\left(z\\right)\\right>]/\\left<u^{all}_*\\left(z\\right)\\right>$', \n",
    "                     fontsize=font_size+2, weight='bold')\n",
    "axes['a'].text(text_loc[0], text_loc[1], '(a)', transform=axes['a'].transAxes + trans,\n",
    "                fontsize=font_size+2, va='bottom')\n",
    "\n",
    "sample_sel = var2d['sel_level']['ldtr']['u_star_dev'][mask1]\n",
    "mask_sel = ~np.isnan(sample_sel)\n",
    "filtered_data_sel = [d[m] for d, m in zip(sample_sel.T, mask_sel.T)]\n",
    "# filtered_data = np.array(filtered_data)\n",
    "axes['b'].boxplot(filtered_data_sel[:12], positions=z, vert=False, widths=0.8,\n",
    "                medianprops=medianprops)\n",
    "axes['b'].set_xlim(-0.6,0.6)\n",
    "axes['b'].set_ylim(0,61)\n",
    "# plot reference lines\n",
    "axes['b'].axvline(x=0, color='grey', linestyle='--', linewidth=1)\n",
    "axes['b'].axvline(x=0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['b'].axvline(x=-0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['b'].axhspan(12, 50.5, facecolor='grey', alpha=0.25)\n",
    "\n",
    "axes['b'].set_yticks(z)\n",
    "axes['b'].tick_params(axis='y', labelsize=font_size+2)\n",
    "axes['b'].tick_params(axis='x', labelsize=font_size+2)\n",
    "axes['b'].set_ylabel(r'$\\it z$ (m)', fontsize=font_size+2)\n",
    "axes['b'].set_xlabel(r'$\\it [u_*\\left(z\\right)-\\left<u^{sel}_*\\left(z\\right)\\right>]/\\left<u^{sel}_*\\left(z\\right)\\right>$', \n",
    "                     fontsize=font_size+2, weight='bold')\n",
    "axes['b'].text(text_loc[0], text_loc[1], '(b)', transform=axes['b'].transAxes + trans,\n",
    "                fontsize=font_size+2, va='bottom')\n",
    "plt.savefig(OUT_PLOT_DIR + 'profile_u_star_ldtr_12l_6l.jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f37170-4a09-49c5-8e07-b5fed0a9a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a107e90-ced8-4d66-ba7c-746402c9b5a8",
   "metadata": {},
   "source": [
    "## high pass filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834b8c7-942c-4af2-b622-e0ede94005c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(6, 5), dpi=150,tight_layout=True)\n",
    "fig, axes = plt.subplot_mosaic([['a','b']], constrained_layout=True)\n",
    "fig.set_size_inches(10, 8)\n",
    "fig.set_dpi(200)\n",
    "# label physical distance in and down:\n",
    "trans = mtransforms.ScaledTranslation(-20/72, 7/72, fig.dpi_scale_trans)\n",
    "medianprops = dict(linestyle='-', linewidth=1, color='k')\n",
    "sample = var2d['all_level']['filt']['u_star_dev'][mask1]\n",
    "mask = ~np.isnan(sample) # Filter data using np.isnan\n",
    "filtered_data = [d[m] for d, m in zip(sample.T, mask.T)]\n",
    "# filtered_data = np.array(filtered_data)\n",
    "axes['a'].boxplot(filtered_data[:12],positions=z, vert=False, widths=0.8,\n",
    "                 medianprops=medianprops)\n",
    "axes['a'].set_xlim(-0.6,0.6)\n",
    "axes['a'].set_ylim(0,61)\n",
    "# plot reference lines\n",
    "axes['a'].axvline(x=0, color='grey', linestyle='--', linewidth=1)\n",
    "axes['a'].axvline(x=0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['a'].axvline(x=-0.25, color='grey', linestyle='--', linewidth=1)\n",
    "# axes['a'].axhspan(12, 50.5, facecolor='grey', alpha=0.25)\n",
    "\n",
    "axes['a'].set_yticks(z)\n",
    "axes['a'].tick_params(axis='y', labelsize=8)\n",
    "axes['a'].set_ylabel(r'\\rm Elevation (m)', fontsize=font_size-1)\n",
    "axes['a'].set_xlabel(r'$\\it (u_*(z)-<u_*^{all}(z)>)/<u_*^{all}(z)>$', fontsize=font_size)\n",
    "axes['a'].text(-0.05, 1, '(a)', transform=axes['a'].transAxes + trans,\n",
    "                fontsize=font_size, va='bottom')\n",
    "\n",
    "sample = var2d['sel_level']['filt']['u_star_dev'][mask1]\n",
    "mask = ~np.isnan(sample) # Filter data using np.isnan\n",
    "filtered_data = [d[m] for d, m in zip(sample.T, mask.T)]\n",
    "# filtered_data = np.array(filtered_data)\n",
    "axes['b'].boxplot(filtered_data[:12],positions=z, vert=False, widths=0.8,\n",
    "                 medianprops=medianprops)\n",
    "axes['b'].set_xlim(-0.6,0.6)\n",
    "axes['b'].set_ylim(0,61)\n",
    "# plot reference lines\n",
    "axes['b'].axvline(x=0, color='grey', linestyle='--', linewidth=1)\n",
    "axes['b'].axvline(x=0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['b'].axvline(x=-0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['b'].axhspan(12, 50.5, facecolor='grey', alpha=0.25)\n",
    "\n",
    "axes['b'].set_yticks(z)\n",
    "axes['b'].tick_params(axis='y', labelsize=8)\n",
    "axes['b'].set_ylabel(r'\\rm Elevation (m)', fontsize=font_size)\n",
    "axes['b'].set_xlabel(r'$\\it (u_*(z)-\\overline{u_*^{sel}(z)})/\\overline{u_*^{sel}(z)}$', fontsize=font_size)\n",
    "axes['b'].text(-0.05, 1, '(b)', transform=axes['b'].transAxes + trans,\n",
    "                fontsize=font_size, va='bottom')\n",
    "# plt.savefig(FIG_DIR + 'profile_u_star_ldtr_12l_6l.jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291fd3f-da18-4460-8dc6-b533df133e77",
   "metadata": {},
   "source": [
    "# Plot PDF of fitted kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba643d2-c462-4f93-aac7-c16e55d99168",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b490556-8c24-4716-89cb-036b9a10c444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "fig, axes = plt.subplot_mosaic([['a','b'],['c','d']], constrained_layout=True)\n",
    "fig.set_size_inches(8,6)\n",
    "fig.set_dpi(300)\n",
    "text_loc = [-0.1, 0.94]\n",
    "font_size = 12\n",
    "# label physical distance in and down:\n",
    "trans = mtransforms.ScaledTranslation(-20/72, 7/72, fig.dpi_scale_trans)\n",
    "# Create a custom legend\n",
    "custom_lines = [Line2D([0], [0], color=\"none\", marker='o', markerfacecolor=\"none\")]\n",
    "\n",
    "def plot_kappa_pdf(fig_lbl,ax_,level):\n",
    "    data = var1d[level]['filt']['kappa_fit'][mask1]\n",
    "    # label='Mean={:.2f}\\nStd={:.2f}'.format(np.nanmean(data), np.nanstd(data))\n",
    "    sns.distplot(data, hist=True, kde=True,\n",
    "                 bins=24, color = 'black', \n",
    "                 hist_kws={'edgecolor':'black',\"range\": [0.2,0.6],'alpha':0.5},\n",
    "                 kde_kws={'linewidth': 0},\n",
    "                 ax=ax_)\n",
    "    # ax_.annotate(label, xy=(0.7,0.8),xycoords='axes fraction',\n",
    "    #              fontsize=font_size-2, \n",
    "    #              bbox={'facecolor': 'white', 'edgecolor': 'gray', 'pad': 1, 'boxstyle': 'round,pad=0.3'})\n",
    "    label = [r'Mean={:.2f}'.format(np.nanmean(data))\n",
    "            +'\\n'+\n",
    "            r'Std={:.2f}'.format(np.nanstd(data))\n",
    "           ]\n",
    "    legend= ax_.legend(custom_lines, label,fontsize=font_size-2, handlelength=-0.5)\n",
    "    # legend.get_frame().set_linewidth(line_width)\n",
    "    ax_.set_xlim(0.15,0.6)\n",
    "    ax_.set_ylim(0,10)\n",
    "    ax_.set_xlabel(r'$\\kappa$', fontsize = font_size)\n",
    "    ax_.set_ylabel(r'PDF', fontsize = font_size)\n",
    "    ax_.tick_params(axis='both', labelsize=font_size)\n",
    "    ax_.axvline(x=0.4, color='k', linestyle='--', linewidth=1)\n",
    "    ax_.text(text_loc[0], text_loc[1], fig_lbl, transform=ax_.transAxes + trans,\n",
    "            fontsize=font_size, zorder=2, \n",
    "            bbox={'facecolor': 'white', 'edgecolor': 'none', 'pad': 1})\n",
    "\n",
    "plot_kappa_pdf('(a)',axes['a'],'test2')\n",
    "# axes['a'].set_title('5th to 11th')\n",
    "    \n",
    "plot_kappa_pdf('(b)',axes['b'],'test1')\n",
    "# axes['b'].set_title('5th to 12th')\n",
    "\n",
    "plot_kappa_pdf('(c)',axes['c'],'sel_level')\n",
    "# axes['c'].set_title('6th to 11th')\n",
    "\n",
    "plot_kappa_pdf('(d)',axes['d'],'test3')\n",
    "# axes['d'].set_title('6th to 12th')\n",
    "\n",
    "plt.savefig(OUT_PLOT_DIR+'PDF_kappa_test.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5422c-9e3f-48e5-8e1f-d65c4d1b2865",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot PDF of R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b658d-d7d7-48f2-9dfa-8f8e40e50184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplot_mosaic([['a','b'],['c','d']], constrained_layout=True)\n",
    "fig.set_size_inches(8,6)\n",
    "fig.set_dpi(200)\n",
    "mask = mask_rnan*mask_dspk*mask_neutral*mask_wdir*mask_taylor*mask_ustar_gt005\n",
    "\n",
    "# sample = r2_ldtr_sel[mask]\n",
    "sample = var1d['sel_level']['filt']['r2_uavg'][mask]\n",
    "label='mean={:.2f}\\nstd={:.2f}'.format(np.nanmean(sample), np.nanstd(sample))\n",
    "sns.distplot(sample, hist=True, kde=True,\n",
    "             bins=24, color = 'black', \n",
    "             hist_kws={'edgecolor':'black',\"range\": [0.8,1],'alpha':0.5},\n",
    "             kde_kws={'linewidth': 0},\n",
    "             label = label,ax=axes['a'])\n",
    "axes['a'].annotate(label, xy=(0.2,0.8),xycoords='axes fraction', fontsize=font_size)\n",
    "axes['a'].set_xlabel(r'$R^2$', fontsize = font_size)\n",
    "axes['a'].set_ylabel(r'PDF', fontsize = font_size)\n",
    "axes['a'].set_title('6th to 11th')\n",
    "axes['a'].set_xlim(0.85,1.05)\n",
    "axes['a'].set_ylim(0,50)\n",
    "\n",
    "# sample = r2_ldtr_test1[mask]\n",
    "sample = var1d['test1']['filt']['r2_uavg'][mask]\n",
    "label='mean={:.2f}\\nstd={:.2f}'.format(np.nanmean(sample), np.nanstd(sample))\n",
    "sns.distplot(sample, hist=True, kde=True,\n",
    "             bins=24, color = 'black', \n",
    "             hist_kws={'edgecolor':'black',\"range\": [0.8,1],'alpha':0.5},\n",
    "             kde_kws={'linewidth': 0},\n",
    "             label = label,ax=axes['b'])\n",
    "axes['b'].annotate(label, xy=(0.2,0.8),xycoords='axes fraction', fontsize=font_size)\n",
    "axes['b'].set_xlabel(r'$R^2$', fontsize = font_size)\n",
    "axes['b'].set_ylabel(r'PDF', fontsize = font_size)\n",
    "axes['b'].set_title('5th to 12th')\n",
    "axes['b'].set_xlim(0.85,1.05)\n",
    "axes['b'].set_ylim(0,50)\n",
    "\n",
    "# sample = r2_ldtr_test2[mask]\n",
    "sample = var1d['test2']['filt']['r2_uavg'][mask]\n",
    "label='mean={:.2f}\\nstd={:.2f}'.format(np.nanmean(sample), np.nanstd(sample))\n",
    "sns.distplot(sample, hist=True, kde=True,\n",
    "             bins=24, color = 'black', \n",
    "             hist_kws={'edgecolor':'black',\"range\": [0.8,1],'alpha':0.5},\n",
    "             kde_kws={'linewidth': 0},\n",
    "             label = label,ax=axes['c'])\n",
    "axes['c'].annotate(label, xy=(0.2,0.8),xycoords='axes fraction', fontsize=font_size)\n",
    "axes['c'].set_xlabel(r'$R^2$', fontsize = font_size)\n",
    "axes['c'].set_ylabel(r'PDF', fontsize = font_size)\n",
    "axes['c'].set_title('5th to 11th')\n",
    "axes['c'].set_xlim(0.85,1.05)\n",
    "axes['c'].set_ylim(0,50)\n",
    "\n",
    "# sample = r2_ldtr_test3[mask]\n",
    "sample = var1d['test3']['filt']['r2_uavg'][mask]\n",
    "label='mean={:.2f}\\nstd={:.2f}'.format(np.nanmean(sample), np.nanstd(sample))\n",
    "sns.distplot(sample, hist=True, kde=True,\n",
    "             bins=24, color = 'black', \n",
    "             hist_kws={'edgecolor':'black',\"range\": [0.8,1],'alpha':0.5},\n",
    "             kde_kws={'linewidth': 0},\n",
    "             label = label,ax=axes['d'])\n",
    "axes['d'].annotate(label, xy=(0.2,0.8),xycoords='axes fraction', fontsize=font_size)\n",
    "axes['d'].set_xlabel(r'$R^2$', fontsize = font_size)\n",
    "axes['d'].set_ylabel(r'PDF', fontsize = font_size)\n",
    "axes['d'].set_title('6th to 12th')\n",
    "axes['d'].set_xlim(0.85,1.05)\n",
    "axes['d'].set_ylim(0,50)\n",
    "\n",
    "# plt.savefig(OUT_PLOT_DIR+'PDF_kappa_test.jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bdba49-1cd3-42e8-969a-110b1c37f86d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot u_star vertical deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb5b82-c521-4ce6-82cb-4b977cb4f016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplot_mosaic([['a','b'],['c','d']], constrained_layout=True)\n",
    "fig.set_size_inches(12, 16)\n",
    "fig.set_dpi(300)\n",
    "medianprops = dict(linestyle='-', linewidth=1, color='k')\n",
    "\n",
    "# subplot a\n",
    "sample = var2d['sel_level']['filt']['u_star_dev'][mask]\n",
    "mask_ = ~np.isnan(sample) # Filter data using np.isnan\n",
    "filtered_data = [d[m] for d, m in zip(sample.T, mask_.T)]\n",
    "# filtered_data = np.array(filtered_data)\n",
    "axes['a'].boxplot(filtered_data[:12],positions=z, vert=False, widths=0.8,\n",
    "                 medianprops=medianprops)\n",
    "axes['a'].set_xlim(-0.6,0.6)\n",
    "axes['a'].set_ylim(0,61)\n",
    "# plot reference lines\n",
    "axes['a'].axvline(x=0, color='grey', linestyle='--', linewidth=1)\n",
    "axes['a'].axvline(x=0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['a'].axvline(x=-0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['a'].axhspan(12, 50.5, facecolor='grey', alpha=0.25)\n",
    "\n",
    "axes['a'].set_ylabel(r'Elevation (m)', fontsize=font_size)\n",
    "axes['a'].set_xlabel(r'$\\it (u_*(z)-\\overline{u_*(z)})/\\overline{u_*(z)}$', fontsize=font_size+4)\n",
    "\n",
    "# subplot b\n",
    "sample = var2d['test1']['filt']['u_star_dev'][mask]\n",
    "mask_ = ~np.isnan(sample) # Filter data using np.isnan\n",
    "filtered_data = [d[m] for d, m in zip(sample.T, mask_.T)]\n",
    "# filtered_data = np.array(filtered_data)\n",
    "axes['b'].boxplot(filtered_data[:12],positions=z, vert=False, widths=0.8,\n",
    "                 medianprops=medianprops)\n",
    "axes['b'].set_xlim(-0.6,0.6)\n",
    "axes['b'].set_ylim(0,61)\n",
    "# plot reference lines\n",
    "axes['b'].axvline(x=0, color='grey', linestyle='--', linewidth=1)\n",
    "axes['b'].axvline(x=0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['b'].axvline(x=-0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['b'].axhspan(12, 50.5, facecolor='grey', alpha=0.25)\n",
    "\n",
    "axes['b'].set_ylabel(r'Elevation (m)', fontsize=font_size)\n",
    "axes['b'].set_xlabel(r'$\\it (u_*(z)-\\overline{u_*(z)})/\\overline{u_*(z)}$', fontsize=font_size+4)\n",
    "\n",
    "# subplot c\n",
    "sample = var2d['test2']['filt']['u_star_dev'][mask]\n",
    "mask_ = ~np.isnan(sample) # Filter data using np.isnan\n",
    "filtered_data = [d[m] for d, m in zip(sample.T, mask_.T)]\n",
    "# filtered_data = np.array(filtered_data)\n",
    "axes['c'].boxplot(filtered_data[:12],positions=z, vert=False, widths=0.8,\n",
    "                 medianprops=medianprops)\n",
    "axes['c'].set_xlim(-0.6,0.6)\n",
    "axes['c'].set_ylim(0,61)\n",
    "# plot reference lines\n",
    "axes['c'].axvline(x=0, color='grey', linestyle='--', linewidth=1)\n",
    "axes['c'].axvline(x=0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['c'].axvline(x=-0.25, color='grey', linestyle='--', linewidth=1)\n",
    "axes['c'].axhspan(12, 50.5, facecolor='grey', alpha=0.25)\n",
    "\n",
    "axes['c'].set_ylabel(r'Elevation (m)', fontsize=font_size)\n",
    "axes['c'].set_xlabel(r'$\\it (u_*(z)-\\overline{u_*(z)})/\\overline{u_*(z)}$', fontsize=font_size+4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260ce77-63a0-4e54-ad67-ca8ff344b193",
   "metadata": {},
   "source": [
    "# *** Do fitting with ISL being decided (6th to 11th level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c33550-e2c1-4d30-9d7c-58d14a029951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable names\n",
    "var1d_names = ['stability_parameter', 'kappa_corrected', 'A1_fit', \n",
    "               'slope_ustd', 'intercept_ustd', 'r2_ustd', 'slope_wstd_ustar','r2_wstd']\n",
    "var2d_names = ['wind_ang_convert','Re_plus', 'IST_u', 'IST_wdir','score_wstd']\n",
    "\n",
    "# Initialize all variables using nested dictionary comprehensions\n",
    "varISL_1d = {type_: {name: np.zeros(n_hours) * np.nan for name in var1d_names} \n",
    "               for type_ in var_types}\n",
    "varISL_2d = {type_: {name: np.zeros((n_hours,sonum)) * np.nan for name in var2d_names} \n",
    "               for type_ in var_types}\n",
    "# score_shear only has (sonum-1) columns\n",
    "varISL_2d['ldtr']['score_shear_wdir'] = np.zeros((n_hours,sonum-1)) * np.nan\n",
    "varISL_2d['ldtr']['score_shear_wspd'] = np.zeros((n_hours,sonum-1)) * np.nan\n",
    "varISL_2d['filt']['score_shear_wdir'] = np.zeros((n_hours,sonum-1)) * np.nan\n",
    "varISL_2d['filt']['score_shear_wspd'] = np.zeros((n_hours,sonum-1)) * np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df461c-dec1-49e9-997f-9eb1bf0afc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "varISL_2d['ldtr'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052ee92-f490-44d5-9c7b-110a4ed15a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "var1d['sel_level']['ldtr']['u_star_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6298057-c552-4e43-8818-161eef2c06cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(n_hours):\n",
    "    if mask_rnan[i]: # skip nan hours (mask_rnan=0)\n",
    "        print(datetime_all[i])\n",
    "        for type_ in var_types:\n",
    "            # Calculate stability parameter with <z>\n",
    "            varISL_1d[type_]['stability_parameter'][i] = gmean(z[list_sel])/ndimage.median(globals()[f'L_H2_{type_}'][i,list_sel])\n",
    "            \n",
    "            # 1. Calculate stability corrected kappa\n",
    "            cal_stability_corrected_kappa(type_, i)\n",
    "            \n",
    "            # Convert wind angle to [-180,180]\n",
    "            varISL_2d[type_]['wind_ang_convert'][i,:] = convert_ang(wind_ang_all[i,:],1)\n",
    "            \n",
    "            # 2. Fit A1 using the selected levels ------------------------------------------------------\n",
    "            cal_ustd_fits(type_, i)\n",
    "            \n",
    "            # 3. Calculate Re_plus\n",
    "            varISL_2d[type_]['Re_plus'][i,:] = z*var1d['sel_level'][type_]['u_star_mean'][i]/nu\n",
    "            \n",
    "            # 4. Calculate IST\n",
    "            # Load turbulent data\n",
    "            strday = str(datetime_all[i][0].strftime(\"%Y%m%d\"))\n",
    "            ih = datetime_all[i][1]\n",
    "            for var_name in in_tur:\n",
    "                globals()[var_name] = np.load(f\"{IN_INS_DIR}{var_name}_{strday}_{ih:02}00.npy\")\n",
    "                \n",
    "            for il in range(sonum): # loop over levels\n",
    "                ## IST(wspd)\n",
    "                varISL_2d[type_]['IST_u'][i,il] = ist_wspd_5min(globals()[f'u_dspk_2rot_{type_}'][:,il])\n",
    "                ## IST(wdir)\n",
    "                varISL_2d[type_]['IST_wdir'][i,il] = ist_wdir(ux_dspk[:,il],uy_dspk[:,il],varISL_2d[type_]['wind_ang_convert'][i,il])\n",
    "            mean_w_std = np.nanmean(globals()[f'w_std_{type_}'][i,list_sel])\n",
    "            varISL_2d[type_]['score_wstd'][i,:] = abs(globals()[f'w_std_{type_}'][i,:]-mean_w_std)/mean_w_std\n",
    "            varISL_2d[type_]['score_shear_wdir'][i,:] = np.diff(varISL_2d[type_]['wind_ang_convert'][i,:])/np.diff(z)\n",
    "            varISL_2d[type_]['score_shear_wspd'][i,:] = np.diff(globals()[f'u_avg_{type_}'][i,:])/np.diff(z)\n",
    "\n",
    "            # 5. Fit w_std against u_star\n",
    "            cal_wstd_fits_through_origin(type_, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3528bd-54a2-4095-9f1b-ca919f5e10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "if write_results:\n",
    "    filename = \"u_std_fitting_1Ddata.pkl\"\n",
    "    with open(OUT_DIR + filename, 'wb') as f:\n",
    "        pickle.dump(varISL_1d, f)\n",
    "            \n",
    "    filename = \"u_std_fitting_2Ddata.pkl\"\n",
    "    with open(OUT_DIR + filename, 'wb') as f:\n",
    "        pickle.dump(varISL_2d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86148535-cb5a-4a3e-aa39-65bf0a8bfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072d5ae-5110-483c-a699-2a5f2c1a3057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(OUT_DIR + \"u_std_fitting_1Ddata.pkl\", 'rb') as file:\n",
    "    varISL_1d = pickle.load(file)\n",
    "for i in range(n_hours):\n",
    "    if mask_rnan[i]: # skip nan hours (mask_rnan=0)\n",
    "        print(datetime_all[i])\n",
    "        for type_ in var_types:\n",
    "            # 5. Fit w_std against u_star\n",
    "            cal_wstd_fits_through_origin(type_, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592fb347-2554-465b-8069-55ebdfe4779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_results:\n",
    "    filename = \"u_std_fitting_1Ddata.pkl\"\n",
    "    with open(OUT_DIR + filename, 'wb') as f:\n",
    "        pickle.dump(varISL_1d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2bff3-69f9-4517-9da2-61d1c5930c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "varISL_1d['filt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11262dbb-d1c7-441d-85de-bfdd7b13027d",
   "metadata": {},
   "source": [
    "# Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cd1b5-41fa-4a0a-b7f2-16b47352e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de445f7-e706-44bc-b0a9-a6d3c40b54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/projectnb/urbanclimate/yueqin/idaho_ec_jupyter/neutral_analysis_data_020624/ist_wdir_neutral.pkl', 'rb') as file:\n",
    "    IST_wdir_neutral = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07405992-b00a-4538-a474-e468b58a125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IST_wdir_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6ec6d-5398-4b32-b496-c3ab8a80bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUT_DIR+'u_std_fitting_2Ddata.pkl', 'rb') as file:\n",
    "    varISL_2d = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
